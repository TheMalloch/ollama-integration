# ü¶ô Ollama Integration - Extension VS Code Avanc√©e

Une extension VS Code de nouvelle g√©n√©ration pour int√©grer Ollama avec analyse intelligente de projet, gestion de contexte persistant et g√©n√©ration de ModelFiles sp√©cialis√©s.

## ‚ú® Fonctionnalit√©s Principales

### üí¨ Chat Int√©gr√© Am√©lior√©
- **Interface Moderne** : Chat intuitive dans la barre lat√©rale VS Code
- **Markdown Avanc√©** : Support complet avec coloration syntaxique Highlight.js
- **S√©lection Mod√®les** : Changement dynamique de mod√®les Ollama
- **Messages Pliables** : Boutons r√©duire/agrandir pour navigation optimis√©e
- **Historique Persistant** : Conservation des conversations entre sessions

### üéØ Envoi de Code Multi-Mode
- **Mode Basique** : Envoi simple du code s√©lectionn√©
- **Mode Contexte Complet** : Inclut imports et fichiers li√©s automatiquement
- **Mode Project-Aware** : Avec analyse compl√®te du projet et d√©pendances
- **Menu Contextuel** : Acc√®s rapide par clic droit

## üÜï Architecture Modulaire Avanc√©e

### üîç Analyse Intelligente de Projet
- **D√©tection Automatique** : Reconnaissance de frameworks (React, Vue, Angular, Express, NestJS, etc.)
- **Architecture Patterns** : Identification MVC, MVVM, Clean Architecture
- **Build Tools** : D√©tection Webpack, Vite, Rollup, Parcel
- **Dependencies Graph** : Analyse compl√®te des d√©pendances locales et externes

### üíæ Gestion de Contexte Persistant
- **Sauvegarde Automatique** : Stockage JSON structur√© dans `.ollama-context/`
- **Structure Organis√©e** : M√©tadonn√©es, d√©pendances, analyse de code
- **Export Multi-LLM** : Formats pour OpenAI, Anthropic, Generic
- **Backup & Restore** : Sauvegarde et restauration du contexte projet

### ü§ñ G√©n√©ration ModelFile Sp√©cialis√©
- **Mode Development** : Optimis√© pour le d√©veloppement actif
- **Mode Debug** : Sp√©cialis√© pour le d√©bogage et r√©solution de probl√®mes
- **Mode Review** : Configur√© pour la revue de code et quality assurance
- **Mode Optimization** : Ax√© sur l'optimisation et les performances

### üìä R√©ponses Adaptatives
- **Token Adaptatif** : Limite dynamique selon la complexit√© du projet
- **Format Structur√©** : R√©ponses organis√©es par sections
- **Contexte Intelligent** : Inclus framework, d√©pendances, et structure

## üéÆ Commandes Disponibles

### Palette de Commandes (Ctrl+Shift+P)
```
üîç Ollama: Analyze Project Structure     ‚Üí Analyse compl√®te du projet
üì§ Ollama: Export Project Context        ‚Üí Export contexte multi-format  
ü§ñ Ollama: Generate Specialized Model    ‚Üí Cr√©ation ModelFile optimis√©
üßπ Ollama: Clear Project Context         ‚Üí Nettoyage du contexte
üìã Ollama: Show Context Summary          ‚Üí R√©sum√© du contexte actuel
```

### Menu Contextuel (Clic droit)
```
üìù Send to Ollama (Basic)               ‚Üí Mode basique original
üéØ Send to Ollama (Full Context)        ‚Üí Avec contexte complet
üß† Send to Ollama (Project Aware)       ‚Üí Avec analyse projet
```

### Anciennes Commandes (R√©trocompatibilit√©)
```
üí¨ Ollama: Open Chat                     ‚Üí Ouvrir l'interface chat
üîÑ Ollama: Clear Chat                    ‚Üí Vider l'historique
‚öôÔ∏è Ollama: Change Model                  ‚Üí Changer le mod√®le actif
```

## ‚öôÔ∏è Configuration Avanc√©e

### Param√®tres Intelligents
```json
{
  // Configuration de base
  "ollama.serverUrl": "http://localhost:11434",
  "ollama.model": "codellama:7b",
  
  // Nouvelles fonctionnalit√©s
  "ollama.useFullContext": true,
  "ollama.showPreviewBeforeSending": false,
  
  // Gestion du contexte
  "ollama.contextStorage.enabled": true,
  "ollama.contextStorage.path": ".ollama-context",
  
  // D√©tection de projet
  "ollama.projectDetection.enabled": true,
  
  // ModelFile automatique
  "ollama.modelFile.autoGenerate": true,
  
  // R√©ponses adaptatives
  "ollama.response.maxTokens": "adaptive",
  "ollama.response.format": "structured"
}
```

### Options de Configuration D√©taill√©es

#### `ollama.response.maxTokens`
- `"adaptive"` : S'ajuste automatiquement selon la taille du projet
- `"1024"`, `"2048"`, `"4096"`, `"8192"` : Limites fixes

#### `ollama.response.format`
- `"structured"` : R√©ponses organis√©es avec sections et hi√©rarchie
- `"markdown"` : Format markdown standard
- `"plain"` : Texte brut simple

#### `ollama.contextStorage.path`
- Chemin relatif au workspace pour stocker le contexte
- Par d√©faut : `.ollama-context`
- Peut √™tre personnalis√© selon l'organisation projet

## üìÅ Structure de Contexte Persistant

### Le Contexte Multi-LLM

Le contexte obtenu est **stock√© et structur√©** pour premi√®rement servir de **'sauvegarde'** mais aussi pour √™tre **utilis√© par d'autres LLM**. Cette approche r√©volutionnaire permet :

1. **Portabilit√©** : M√™me analyse pour OpenAI, Anthropic, Ollama
2. **Efficacit√©** : Pas de re-analyse √† chaque utilisation
3. **Consistance** : Contexte identique entre diff√©rents outils
4. **Backup** : Sauvegarde compl√®te de l'√©tat du projet

### Fichier `.ollama-context/project-context.json`
```json
{
  "metadata": {
    "type": "web|mobile|desktop|library|api|cli",
    "language": "typescript|javascript|python|...",
    "frameworks": [
      {
        "name": "React",
        "version": "18.2.0", 
        "type": "frontend",
        "confidence": 95,
        "indicators": ["package.json", "jsx files"],
        "patterns": ["src/components/", "public/index.html"]
      }
    ],
    "buildTools": ["vite", "typescript", "eslint"],
    "testFrameworks": ["jest", "testing-library"],
    "architecture": ["component-based", "hooks-pattern"],
    "packageManager": "npm|yarn|pnpm"
  },
  "timestamp": "2024-01-15T10:30:00Z",
  "version": "1.0.0",
  "workspace": "/path/to/project",
  
  "dependencies": {
    "local": [
      {
        "path": "./src/components/Button.tsx",
        "imports": ["React", "./types"],
        "exports": ["Button", "ButtonProps"],
        "usages": ["./src/pages/Home.tsx"]
      }
    ],
    "external": [
      {
        "name": "react",
        "version": "18.2.0",
        "type": "dependency",
        "usedIn": ["components", "hooks"],
        "importance": "critical"
      }
    ],
    "tree": [
      {
        "name": "src/App.tsx",
        "children": ["src/components/", "src/hooks/"],
        "depth": 0
      }
    ]
  },
  
  "structure": {
    "components": [
      {
        "path": "src/components/Button.tsx",
        "type": "functional-component",
        "props": ["onClick", "children", "variant"],
        "hooks": ["useState", "useCallback"]
      }
    ],
    "services": [
      {
        "path": "src/services/api.ts", 
        "exports": ["fetchUser", "createPost"],
        "dependencies": ["axios"]
      }
    ],
    "configuration": [
      {
        "file": "vite.config.ts",
        "type": "build-config",
        "settings": ["plugins", "resolve", "build"]
      }
    ],
    "tests": [
      {
        "path": "src/__tests__/Button.test.tsx",
        "targets": ["src/components/Button.tsx"],
        "framework": "jest"
      }
    ]
  },
  
  "analysis": [
    {
      "timestamp": "2024-01-15T10:30:00Z",
      "type": "dependency-analysis",
      "duration": "1.2s",
      "results": {
        "totalFiles": 45,
        "codeFiles": 38,
        "testFiles": 7,
        "complexity": "medium"
      },
      "summary": "Projet React TypeScript avec Vite - Architecture moderne"
    }
  ],
  
  "exports": {
    "ollama": "FROM codellama:7b\\nSYSTEM You are a React TypeScript expert...",
    "openai": {
      "system_prompt": "Tu es un expert React TypeScript sp√©cialis√©...",
      "context": {
        "project_type": "React TypeScript avec Vite",
        "frameworks": ["React", "TypeScript", "Vite"],
        "key_files": ["src/App.tsx", "src/components/"],
        "dependencies": ["react", "typescript", "vite"]
      },
      "instructions": [
        "Toujours fournir du code TypeScript typ√©",
        "Utiliser les patterns React modernes (hooks)",
        "Respecter l'architecture existante"
      ]
    },
    "anthropic": {
      "system": "Vous √™tes un assistant expert en d√©veloppement React TypeScript...",
      "context": {
        "project_analysis": {
          "type": "frontend-web-app",
          "stack": ["React", "TypeScript", "Vite"],
          "patterns": ["functional-components", "custom-hooks"],
          "structure": "src/ based with components separation"
        }
      },
      "guidelines": [
        "Prioriser la lisibilit√© et la maintenabilit√©",
        "Utiliser TypeScript de mani√®re idiomatique",
        "Suivre les conventions React √©tablies"
      ]
    },
    "generic": {
      "description": "Modern React TypeScript project with Vite build tool",
      "technologies": ["React 18.2.0", "TypeScript 4.9+", "Vite 4.0+"],
      "architecture": "Component-based with functional patterns",
      "context_data": {
        "file_count": 45,
        "main_directories": ["src/components", "src/hooks", "src/services"],
        "test_coverage": "Jest + React Testing Library",
        "styling": "CSS Modules + Tailwind CSS"
      },
      "recommendations": [
        "Use functional components with hooks",
        "Maintain strong TypeScript typing",
        "Follow established project structure",
        "Prioritize performance and accessibility"
      ]
    }
  }
}
```

### B√©n√©fices de cette Structure

‚úÖ **Sauvegarde Compl√®te** : Toute l'analyse du projet est pr√©serv√©e  
‚úÖ **R√©utilisabilit√©** : M√™me contexte pour diff√©rents LLM  
‚úÖ **√âvolutivit√©** : Structure extensible pour nouvelles analyses  
‚úÖ **Performance** : Cache intelligent √©vite re-analyse inutile  
‚úÖ **Portabilit√©** : Format JSON standard facilement exportable  
‚úÖ **Versionning** : Suivi des changements d'analyse dans le temps  

## üîÑ Utilisation avec Autres LLM

### Export Automatique

L'extension g√©n√®re automatiquement les formats pour diff√©rents providers :

```bash
.ollama-context/
‚îú‚îÄ‚îÄ project-context.json       # Contexte complet
‚îú‚îÄ‚îÄ exports/
‚îÇ   ‚îú‚îÄ‚îÄ ollama-modelfile       # ModelFile Ollama
‚îÇ   ‚îú‚îÄ‚îÄ openai-context.json    # Format OpenAI
‚îÇ   ‚îú‚îÄ‚îÄ anthropic-context.json # Format Claude
‚îÇ   ‚îî‚îÄ‚îÄ generic-context.json   # Format g√©n√©rique
```

### Utilisation Pratique

#### Avec OpenAI
```bash
curl -X POST https://api.openai.com/v1/chat/completions \\
  -H "Authorization: Bearer $OPENAI_API_KEY" \\
  -d @.ollama-context/exports/openai-context.json
```

#### Avec Claude (Anthropic)
```bash
curl -X POST https://api.anthropic.com/v1/messages \\
  -H "x-api-key: $ANTHROPIC_API_KEY" \\
  -d @.ollama-context/exports/anthropic-context.json
```

#### Avec Ollama (ModelFile)
```bash
ollama create my-project-expert -f .ollama-context/exports/ollama-modelfile
ollama run my-project-expert
```

## üéØ Exemples d'Usage Avanc√©s

### D√©veloppement avec Contexte Intelligent

Quand vous envoyez du code avec "Project Aware", l'extension inclut automatiquement :

- ‚úÖ Type de projet d√©tect√© (React/Vue/Angular/etc.)
- ‚úÖ D√©pendances install√©es et utilis√©es
- ‚úÖ Patterns architecturaux identifi√©s
- ‚úÖ Structure des fichiers et organisation
- ‚úÖ Configuration des outils de build
- ‚úÖ Frameworks de test utilis√©s

**R√©sultat** : R√©ponses pr√©cises et parfaitement adapt√©es √† votre projet !

### Analyse Multi-Projet

Chaque workspace a son contexte ind√©pendant :

```bash
workspace/
‚îú‚îÄ‚îÄ frontend-react/
‚îÇ   ‚îî‚îÄ‚îÄ .ollama-context/          # Contexte React
‚îú‚îÄ‚îÄ backend-node/
‚îÇ   ‚îî‚îÄ‚îÄ .ollama-context/          # Contexte Node.js
‚îî‚îÄ‚îÄ mobile-react-native/
    ‚îî‚îÄ‚îÄ .ollama-context/          # Contexte React Native
```

### G√©n√©ration ModelFile Sp√©cialis√©

Exemple de ModelFile g√©n√©r√© automatiquement :

```dockerfile
FROM codellama:7b

SYSTEM You are a React TypeScript expert specialized in modern web development.

Project Context:
- Framework: React 18.2.0 with TypeScript 4.9+
- Build Tool: Vite 4.0+ with Hot Module Replacement
- State Management: Redux Toolkit + RTK Query  
- Styling: Tailwind CSS 3.0+ with CSS Modules
- Testing: Jest + React Testing Library + MSW
- Architecture: Component-based with custom hooks pattern

Current Project Structure:
- src/components/ - Reusable UI components
- src/hooks/ - Custom React hooks
- src/services/ - API services and data fetching
- src/utils/ - Utility functions and helpers
- src/types/ - TypeScript type definitions

Focus Areas:
- Modern React patterns (functional components, hooks)
- TypeScript best practices and strict type safety
- Performance optimization (memo, useMemo, useCallback)
- Responsive design with Tailwind utilities
- Clean, maintainable, and testable code structure
- Accessibility (a11y) best practices

Code Style Guidelines:
- Use functional components with TypeScript
- Implement proper error boundaries
- Follow the established project patterns
- Prioritize readability and maintainability
- Include proper TypeScript types and interfaces

Always provide code examples that fit this project structure and follow the established architectural patterns.

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
```

## üöÄ Installation et Configuration

### 1. Pr√©requis

- VS Code 1.70.0+
- Node.js 16+ (pour le d√©veloppement)
- Ollama install√© et en cours d'ex√©cution

### 2. Installation Extension

#### Depuis VS Code Marketplace
1. Ouvrir VS Code
2. Aller dans Extensions (Ctrl+Shift+X)
3. Rechercher "Ollama Integration"
4. Cliquer "Install"

#### Installation Manuelle
```bash
# Cloner le repository
git clone <repository-url>
cd ollama-integration

# Installer les d√©pendances
npm install

# Compiler
npm run compile

# Packager (optionnel)
vsce package
```

### 3. Configuration Initiale

1. **D√©marrer Ollama** :
```bash
ollama serve
```

2. **Configurer VS Code** :
   - Ouvrir Settings (Ctrl+,)
   - Rechercher "ollama"
   - Configurer `ollama.serverUrl` si diff√©rent de localhost
   - Activer `ollama.useFullContext` pour l'analyse compl√®te

3. **Premier Test** :
   - Ouvrir Command Palette (Ctrl+Shift+P)
   - Lancer "Ollama: Analyze Project Structure"
   - V√©rifier la cr√©ation de `.ollama-context/`

## üîß Architecture Technique

### Structure Modulaire

```
src/
‚îú‚îÄ‚îÄ extension.ts                 # Point d'entr√©e principal
‚îú‚îÄ‚îÄ interfaces/
‚îÇ   ‚îî‚îÄ‚îÄ IChatProvider.ts        # Contrats TypeScript
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ context/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ contextManager.ts   # Gestion contexte persistant
‚îÇ   ‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ modelFileGenerator.ts # G√©n√©ration ModelFiles
‚îÇ   ‚îî‚îÄ‚îÄ analysis/
‚îÇ       ‚îî‚îÄ‚îÄ codeAnalyzer.ts     # Analyse code et d√©pendances
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ projectDetector.ts      # D√©tection frameworks/projets
‚îÇ   ‚îî‚îÄ‚îÄ fileUtils.ts           # Utilitaires syst√®me fichiers
‚îú‚îÄ‚îÄ chatProvider.ts             # Interface chat WebView
‚îî‚îÄ‚îÄ ollamaService.ts           # Service API Ollama
```

### Design Patterns Utilis√©s

- **Modular Architecture** : S√©paration claire des responsabilit√©s
- **Dependency Injection** : Services d√©coupl√©s et testables  
- **Observer Pattern** : Mise √† jour r√©active du contexte
- **Strategy Pattern** : Multiples modes d'analyse et export
- **Factory Pattern** : G√©n√©ration dynamique de ModelFiles

### Technologies Int√©gr√©es

- **TypeScript** : Typage strict et interfaces robustes
- **VS Code API** : WebView, Commands, Configuration
- **Node.js FS** : Analyse syst√®me de fichiers
- **JSON Schema** : Validation structure contexte
- **Markdown Rendering** : Affichage riche dans le chat

## üêõ D√©pannage

### Probl√®mes Courants

#### ‚ùå Contexte Non G√©n√©r√©
**Sympt√¥mes** : Pas de dossier `.ollama-context/` cr√©√©

**Solutions** :
- V√©rifier permissions d'√©criture dans le workspace
- S'assurer que `ollama.contextStorage.enabled: true`
- Red√©marrer VS Code apr√®s changement de config
- V√©rifier que le workspace contient des fichiers de code

#### ‚ùå D√©tection Projet √âchoue  
**Sympt√¥mes** : Type de projet d√©tect√© comme "unknown"

**Solutions** :
- V√©rifier pr√©sence de `package.json`, `requirements.txt`, ou autres fichiers de config
- Consulter les logs dans Output Panel ‚Üí "Ollama Integration"
- Activer `ollama.debug: true` pour traces d√©taill√©es
- S'assurer que les fichiers ne sont pas dans `.gitignore`

#### ‚ùå ModelFile Non Cr√©√©
**Sympt√¥mes** : Pas de ModelFile g√©n√©r√© dans les exports

**Solutions** :
- V√©rifier connexion au serveur Ollama (`ollama serve`)
- S'assurer que le mod√®le de base est disponible (`ollama list`)
- V√©rifier `ollama.modelFile.autoGenerate: true`
- Contr√¥ler les permissions d'√©criture

#### ‚ùå Erreur "Module Not Found"
**Sympt√¥mes** : Erreurs TypeScript ou imports manquants

**Solutions** :
```bash
# R√©installer les d√©pendances
rm -rf node_modules
npm install

# Recompiler
npm run compile

# Red√©marrer VS Code
```

### Logs et Debug

#### Activation Debug Mode
```json
{
  "ollama.debug": true,
  "ollama.verbose": true
}
```

#### Consultation des Logs
1. Ouvrir Output Panel (Ctrl+Shift+U)
2. S√©lectionner "Ollama Integration" dans le dropdown
3. Analyser les messages d'erreur et warnings

#### Logs Utiles
```bash
# Logs d√©taill√©s de l'analyse
[Ollama] Analyzing project structure...
[Ollama] Detected frameworks: React, TypeScript
[Ollama] Generated context: 45 files, 2.3MB

# Logs d'erreur
[Ollama] Error: Cannot write to .ollama-context/
[Ollama] Solution: Check write permissions
```

### Reset Complet

Si tous les probl√®mes persistent :

```bash
# 1. Supprimer le contexte existant
rm -rf .ollama-context/

# 2. Reset configuration VS Code
# Settings ‚Üí Extensions ‚Üí Ollama ‚Üí "Reset to Defaults"

# 3. Red√©marrer compl√®tement
# Fermer VS Code, red√©marrer Ollama, rouvrir VS Code
```

## üîÆ Roadmap et Prochaines Fonctionnalit√©s

### Version 2.0 (Q2 2024)
- [ ] **Interface Graphique ModelFile** : √âditeur visuel pour personnaliser les ModelFiles
- [ ] **Templates Personnalisables** : Biblioth√®que de templates pour diff√©rents use cases
- [ ] **Export Cloud** : Sauvegarde contexte vers services cloud (GitHub, GitLab)
- [ ] **Collaboration** : Partage de contexte entre √©quipes

### Version 2.1 (Q3 2024)  
- [ ] **Analyse S√©mantique** : Compr√©hension du code plus profonde
- [ ] **Git Integration** : Contexte bas√© sur l'historique Git
- [ ] **Multi-Workspace** : Gestion de projets multiples simultan√©s
- [ ] **Performance Dashboard** : M√©triques d'utilisation et optimisation

### Version 3.0 (Q4 2024)
- [ ] **AI-Powered Suggestions** : Suggestions intelligentes de ModelFiles
- [ ] **Real-time Collaboration** : Collaboration en temps r√©el sur le contexte
- [ ] **Plugin Ecosystem** : API pour extensions tierces
- [ ] **Enterprise Features** : SSO, audit logs, compliance

## ü§ù Contribution

### D√©veloppement Local

```bash
# Cloner et setup
git clone <repository-url>
cd ollama-integration
npm install

# D√©veloppement avec watch
npm run watch

# Test dans VS Code
# Presser F5 pour lancer Extension Development Host
```

### Structure de Contribution

Le code est maintenant **modulaire et extensible**. Chaque composant peut √™tre d√©velopp√© ind√©pendamment :

- **`utils/`** : D√©tection et analyse (ajouter nouveaux frameworks)
- **`core/`** : Logique m√©tier principale (nouveaux modes d'analyse)
- **`interfaces/`** : Contrats TypeScript (nouvelles int√©grations)

### Guidelines

1. **Suivre TypeScript Strict** : Typage complet requis
2. **Tests Unitaires** : Couvrir les nouvelles fonctionnalit√©s
3. **Documentation** : Commenter le code complexe
4. **Performance** : Optimiser pour gros projets

## üìû Support et Communaut√©

### Support Technique
- üìö **Documentation** : Consulter ce README et [MIGRATION.md](./MIGRATION.md)
- üîç **Debug** : Utiliser les logs dans Output Panel
- üß™ **Test** : Utiliser `test-features.js` pour diagnostics
- üìù **Comparison** : Comparer avec `extension.old.ts` si migration

### Communaut√©
- üêõ **Issues** : Reporter bugs sur GitHub
- üí° **Feature Requests** : Proposer nouvelles fonctionnalit√©s  
- üìñ **Wiki** : Contribuer √† la documentation
- üí¨ **Discussions** : √âchanger avec la communaut√©

### Contact
- üìß **Email** : support@ollama-integration
- üê¶ **Twitter** : @ollama_vscode
- üí¨ **Discord** : [Lien du serveur](discord-link)

---

## üéâ Conclusion

L'**Ollama Integration** avec sa nouvelle architecture modulaire repr√©sente une √©volution majeure dans l'int√©gration d'IA dans VS Code. 

### Points Forts

‚úÖ **Contexte Intelligent** : Analyse automatique et compl√®te du projet  
‚úÖ **Multi-LLM Support** : Compatible OpenAI, Anthropic, Ollama  
‚úÖ **Sauvegarde Persistante** : Pas de perte de contexte entre sessions  
‚úÖ **ModelFiles Sp√©cialis√©s** : IA optimis√©e pour votre projet sp√©cifique  
‚úÖ **Architecture Modulaire** : Facilement extensible et maintenable  
‚úÖ **Performance Optimis√©e** : Cache intelligent et traitement adaptatif  

### Innovation

üöÄ **Premi√®re extension VS Code** √† offrir un contexte v√©ritablement **multi-LLM** avec sauvegarde persistante et g√©n√©ration automatique de ModelFiles sp√©cialis√©s.

**L'√®re de l'IA contextuelle intelligente commence maintenant ! üåü**

---

*D√©velopp√© avec ‚ù§Ô∏è pour la communaut√© des d√©veloppeurs. Contribuez et faisons √©voluer l'IA dans le d√©veloppement !*
