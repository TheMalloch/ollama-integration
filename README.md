# ğŸ¦™ Ollama Integration - VS Code Extension

Extension VS Code pour intÃ©gration Ollama avec analyse contextuelle intelligente.

## âœ… FonctionnalitÃ©s ImplÃ©mentÃ©es

### Interface Chat
- [x] Chat sidebar avec WebView
- [x] Messages avec boutons rÃ©duire/agrandir
- [x] Support Markdown + highlight.js
- [x] SÃ©lection dynamique modÃ¨les Ollama
- [x] Historique persistant conversations

### Analyse de Code
- [x] Envoi code sÃ©lectionnÃ© (mode basique)
- [x] Analyse contextuelle complÃ¨te (imports + dÃ©pendances)
- [x] Menu contextuel clic droit
- [x] PrÃ©visualisation message avant envoi
- [x] DÃ©tection imports locaux/externes
- [x] Graphe dÃ©pendances fichiers
- [x] Inclusion code pertinent automatique

### Configuration
- [x] URL serveur configurable (`ollama.serverUrl`)
- [x] SÃ©lection modÃ¨le (`ollama.model`)
- [x] Toggle contexte complet (`ollama.useFullContext`)
- [x] Mode prÃ©visualisation (`ollama.showPreviewBeforeSending`)

## ğŸ”„ Roadmap DÃ©veloppement

### Phase 1: Modularisation (PrioritÃ©)
- [ ] Refactoring `extension.ts` (1515 lignes â†’ modules)
- [ ] Extraction `core/analysis/` - moteur analyse
- [ ] Extraction `core/context/` - gestion contexte  
- [ ] Extraction `utils/` - utilitaires partagÃ©s
- [ ] DÃ©finition interfaces TypeScript strictes

### Phase 2: Contexte Persistant
- [ ] SystÃ¨me sauvegarde `.ollama-context/`
- [ ] Structure JSON rÃ©utilisable multi-LLM
- [ ] Export OpenAI/Anthropic/Generic
- [ ] Restauration automatique contexte
- [ ] Versioning contexte projet

### Phase 3: Intelligence Projet
- [ ] DÃ©tection frameworks (React/Vue/Angular/Express/etc.)
- [ ] GÃ©nÃ©ration ModelFiles spÃ©cialisÃ©s selon projet
- [ ] Templates contextuels adaptÃ©s architecture
- [ ] Recommandation LLM selon capacitÃ©s machine
- [ ] Optimisation paramÃ¨tres selon RAM/CPU

### Phase 4: Assistant IA AvancÃ©
- [ ] AutocomplÃ©tion intelligente temps rÃ©el
- [ ] GÃ©nÃ©ration code (fonctions/classes/modules)
- [ ] Refactoring assistÃ© automatique
- [ ] Documentation automatique
- [ ] Tests unitaires gÃ©nÃ©rÃ©s
- [ ] Correction erreurs + suggestions
- [ ] Conversion langages programmation

### Phase 5: Performance & ExtensibilitÃ©
- [ ] Cache intelligent analyses
- [ ] Support multi-workspace
- [ ] Interface multilingue (FR/EN/ES/DE)
- [ ] Plugin ecosystem / API extensibilitÃ©
- [ ] MÃ©triques usage et optimisation

## ğŸ—ï¸ Architecture Actuelle

```
src/
â”œâ”€â”€ extension.ts      # 1515L - Logique principale (Ã  refactorer)
â”œâ”€â”€ chatProvider.ts   # Interface WebView chat
â””â”€â”€ ollamaService.ts  # API Ollama communication
```

## ğŸ¯ Architecture Cible

```
src/
â”œâ”€â”€ extension.ts           # Point d'entrÃ©e lÃ©ger
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ analysis/         # Moteur analyse code
â”‚   â”œâ”€â”€ context/          # Gestion contexte persistant
â”‚   â””â”€â”€ llm/             # IntÃ©gration LLM + ModelFiles
â”œâ”€â”€ providers/
â”‚   â””â”€â”€ chatProvider.ts   # Interface chat
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ projectDetector.ts # DÃ©tection frameworks
â”‚   â””â”€â”€ fileUtils.ts      # Utilitaires systÃ¨me
â””â”€â”€ interfaces/
    â””â”€â”€ types.ts          # DÃ©finitions TypeScript
```

## âš™ï¸ Config Dev

```bash
# Setup
npm install
npm run compile
npm run watch  # Dev mode

# Test
F5  # Extension Development Host
```

## ğŸ“‹ Commandes Disponibles

```
ollama.sendToChat           # Envoi avec contexte
ollama.previewMessage       # PrÃ©visualisation  
ollama.toggleFullContext   # Toggle analyse
ollama.clearChat           # Clear historique
```

## ğŸ¯ Objectif Final

**Assistant IA local complet** reproduisant capacitÃ©s GitHub Copilot/Cursor mais:
- ğŸ  **100% local** (confidentialitÃ© totale)
- ğŸ§  **Contextuel** (mÃ©moire projet persistante) 
- ğŸ”„ **Multi-LLM** (rÃ©utilisable OpenAI/Anthropic)
- ğŸ’° **Gratuit** (coÃ»t zÃ©ro)


## ğŸ’¡ Vision Future : Contexte Multi-LLM

### Concept Central
**"Le contexte obtenu doit Ãªtre stockÃ© et structurÃ© pour premiÃ¨rement servir de 'sauvegarde' mais aussi pour Ãªtre utilisÃ© par d'autres LLM"**

### Structure de Contexte EnvisagÃ©e
```json
{
  "metadata": {
    "project_type": "web|mobile|desktop|library",
    "framework": "react|vue|angular|express|...",
    "language": "typescript|javascript|python|...",
    "analysis_date": "2024-01-15T10:30:00Z"
  },
  "dependencies": {
    "local": [
      {
        "path": "./src/components/Button.tsx",
        "imports": ["React", "./types"],
        "exports": ["Button", "ButtonProps"]
      }
    ],
    "external": [
      {
        "name": "react",
        "version": "18.2.0",
        "usage": "critical"
      }
    ]
  },
  "structure": {
    "components": ["Button", "Header", "Layout"],
    "services": ["api", "auth", "storage"],
    "utils": ["helpers", "constants"]
  },
  "exports": {
    "ollama": "FROM codellama:7b\\nSYSTEM Tu es un expert React...",
    "openai": {
      "system_prompt": "Tu es un assistant spÃ©cialisÃ© en React...",
      "context": {...}
    },
    "anthropic": {
      "system": "Vous Ãªtes un expert en dÃ©veloppement React...",
      "context": {...}
    }
  }
}
```

### Utilisation Multi-LLM
```bash
# Export pour OpenAI
curl -X POST https://api.openai.com/v1/chat/completions \\
  -d @.ollama-context/openai-export.json

# Export pour Claude
curl -X POST https://api.anthropic.com/v1/messages \\
  -d @.ollama-context/anthropic-export.json

# ModelFile pour Ollama
ollama create my-project-expert -f .ollama-context/modelfile
```

## ğŸ¤ Contribution

### Ã‰tat Actuel
- âœ… Interface chat fonctionnelle avec boutons rÃ©duire
- âœ… Analyse contextuelle de base
- âœ… IntÃ©gration Ollama complÃ¨te
- âœ… Configuration flexible

### Prochaines Contributions SouhaitÃ©es
- ğŸ”„ Modularisation du code (fichier trop long)
- ğŸ’¾ ImplÃ©mentation du systÃ¨me de contexte persistant
- ğŸ¤– GÃ©nÃ©ration automatique de ModelFiles dÃ©pendament du context du projet 
- ğŸ“Š RÃ©ponses adaptatives selon la complexitÃ©

---

## ğŸ“ Support et Feedback

Cette extension Ã©volue selon les besoins rÃ©els d'utilisation. N'hÃ©sitez pas Ã  :
- Reporter des bugs ou problÃ¨mes rencontrÃ©s
- SuggÃ©rer des amÃ©liorations basÃ©es sur votre workflow
- Partager des exemples d'usage avec diffÃ©rents types de projets
- Contribuer au dÃ©veloppement des fonctionnalitÃ©s planifiÃ©es

**L'objectif est de crÃ©er l'extension d'assistant IA local la plus avancÃ©e possible, offrant toutes les fonctionnalitÃ©s des assistants de code existants tout en restant entiÃ¨rement contextuelle et rÃ©utilisable ! ğŸš€**

### ğŸ¯ Vision : Assistant IA Local Complet

Cette extension vise Ã  reproduire et amÃ©liorer les capacitÃ©s des assistants IA populaires :

#### ğŸ”¥ FonctionnalitÃ©s d'Assistant de Code VisÃ©es
- **AutocomplÃ©tion intelligente** : Suggestions de code en temps rÃ©el
- **GÃ©nÃ©ration de code** : CrÃ©ation de fonctions, classes et modules complets  
- **Refactoring assistÃ©** : AmÃ©lioration et restructuration automatique
- **Documentation automatique** : GÃ©nÃ©ration de commentaires et docs
- **Tests unitaires** : CrÃ©ation automatique de tests basÃ©s sur le code
- **Correction d'erreurs** : DÃ©tection et suggestions de correction
- **Explication de code** : Analyse et explications dÃ©taillÃ©es
- **Conversion de langages** : Translation entre diffÃ©rents langages de programmation

#### ğŸ  Avantages du Local
- **ConfidentialitÃ© totale** : Aucune donnÃ©e envoyÃ©e vers des serveurs externes
- **Personnalisation** : ModÃ¨les adaptÃ©s spÃ©cifiquement Ã  votre projet
- **Performance** : Latence minimale avec Ollama local
- **CoÃ»t zÃ©ro** : Pas d'abonnement ou de tokens payants
- **DisponibilitÃ©** : Fonctionne mÃªme hors ligne

#### ğŸ”„ ContextualitÃ© et RÃ©utilisabilitÃ©
- **Apprentissage continu** : Le contexte s'enrichit Ã  chaque utilisation
- **MÃ©moire de projet** : L'assistant "connaÃ®t" votre codebase
- **Patterns personnalisÃ©s** : DÃ©tection de vos conventions de codage
- **Export universel** : Contexte rÃ©utilisable avec d'autres LLMs
- **Ã‰volution adaptative** : L'assistant s'amÃ©liore avec le projet
