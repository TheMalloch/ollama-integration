# ğŸ¦™ Ollama Integration - Extension VS Code

Extension VS Code pour intÃ©grer Ollama avec interface chat dans la barre latÃ©rale et fonctionnalitÃ©s avancÃ©es d'analyse de code contextuelle.

## âœ¨ FonctionnalitÃ©s Actuelles

### ğŸ’¬ Interface Chat IntÃ©grÃ©e
- **Chat dans la sidebar** : Interface de chat directement dans la barre latÃ©rale VS Code
- **Messages avec boutons rÃ©duire** : Comme demandÃ© - "fais en sorte que le bouton rÃ©duire soit en bas du message et que la rÃ©ponse elle mÃªme est un bouton rÃ©duire"
- **Support Markdown** : Rendu complet avec coloration syntaxique
- **SÃ©lection de modÃ¨les** : Changement dynamique des modÃ¨les Ollama
- **Historique persistant** : Conservation des conversations

### ğŸ¯ Envoi de Code Intelligent
- **Mode Basique** : Envoi simple du code sÃ©lectionnÃ©
- **Mode Contexte Complet** : Analyse avancÃ©e avec imports et dÃ©pendances
- **Menu contextuel** : Clic droit pour "Envoyer vers Ollama"
- **PrÃ©visualisation** : Option pour voir le message avant envoi

### ğŸ” Analyse Contextuelle AvancÃ©e
- **Analyse des imports** : DÃ©tection automatique des dÃ©pendances locales et externes
- **Suivi des dÃ©pendances** : Graphe complet des relations entre fichiers
- **Analyse de structure** : ComprÃ©hension de l'architecture du projet
- **Code pertinent** : Inclusion intelligente du code liÃ© au contexte

## ğŸš€ FonctionnalitÃ©s DiscutÃ©es (Roadmap)

### ğŸ’¾ SystÃ¨me de Contexte Persistant
Comme mentionnÃ© : **"ajoute au readme que le contexte obtenu doit Ãªtre stockÃ© et structurÃ© pour premiÃ¨rement servir de 'sauvegarde' mais aussi pour Ãªtre utilisÃ© par d'autres LLM"**

- **Sauvegarde du contexte** : Stockage automatique dans `.ollama-context/`
- **Structure pour multi-LLM** : Format compatible OpenAI, Anthropic, etc.
- **Backup intelligent** : PrÃ©servation de l'analyse entre sessions
- **Export/Import** : RÃ©utilisation du contexte avec d'autres outils

### ğŸ”§ Architecture Modulaire
DemandÃ©e car **"le fichier devient trop long"** :

```
src/
â”œâ”€â”€ extension.ts           # Point d'entrÃ©e principal (version actuelle)
â”œâ”€â”€ chatProvider.ts        # Interface chat
â”œâ”€â”€ ollamaService.ts       # Service API Ollama
â””â”€â”€ Modules planifiÃ©s:
    â”œâ”€â”€ core/analysis/     # Moteur d'analyse du code
    â”œâ”€â”€ core/context/      # Gestion contexte et sauvegarde
    â”œâ”€â”€ utils/             # Utilitaires et dÃ©tection projet
```

### ğŸ¤– GÃ©nÃ©ration ModelFile SpÃ©cialisÃ©
- **ModelFile automatique** : GÃ©nÃ©ration basÃ©e sur l'analyse du projet
- **Prompts spÃ©cialisÃ©s** : AdaptÃ©s au framework dÃ©tectÃ©
- **Templates contextuels** : PersonnalisÃ©s selon l'architecture

### ğŸ“Š RÃ©ponses Adaptatives
- **Limite adaptative** : Taille des rÃ©ponses selon la complexitÃ© du projet
- **Format structurÃ©** : Organisation claire des rÃ©ponses longues
- **Compression intelligente** : Optimisation pour les gros projets

## ğŸ® Commandes Disponibles

### Dans la Palette (Ctrl+Shift+P)
```
ğŸ”¹ Ollama: Envoyer vers Ollama          â†’ Envoi avec analyse contextuelle
ğŸ”¹ Ollama: PrÃ©visualiser le message     â†’ Voir avant d'envoyer
ğŸ”¹ Ollama: Activer/DÃ©sactiver contexte  â†’ Toggle analyse complÃ¨te
ğŸ”¹ Ollama: Effacer le chat              â†’ Vider l'historique
```

### Menu Contextuel (Clic droit)
```
ğŸ“ Envoyer vers Ollama                  â†’ Sur code sÃ©lectionnÃ©
ğŸ” PrÃ©visualiser le message pour Ollama â†’ Avec prÃ©visualisation
```

## âš™ï¸ Configuration

### ParamÃ¨tres Actuels
```json
{
  "ollama.serverUrl": "http://localhost:11434",
  "ollama.model": "codellama:7b", 
  "ollama.useFullContext": true,
  "ollama.showPreviewBeforeSending": false
}
```

### Configuration AvancÃ©e (PlanifiÃ©e)
```json
{
  "ollama.contextStorage.enabled": true,
  "ollama.contextStorage.path": ".ollama-context",
  "ollama.projectDetection.enabled": true,
  "ollama.modelFile.autoGenerate": true,
  "ollama.response.maxTokens": "adaptive",
  "ollama.response.format": "structured"
}
```

## ğŸ› ï¸ Installation et Utilisation

### PrÃ©requis
- VS Code 1.74.0+
- Ollama installÃ© et fonctionnel (`ollama serve`)
- Node.js pour le dÃ©veloppement

### Installation
1. Cloner le repository
2. `npm install`
3. `npm run compile`
4. F5 pour tester dans Extension Development Host

### Utilisation Basique
1. **Ouvrir le chat** : L'icÃ´ne Ollama apparaÃ®t dans la barre d'activitÃ©
2. **Envoyer du code** : SÃ©lectionner du code â†’ clic droit â†’ "Envoyer vers Ollama"
3. **Messages rÃ©ductibles** : Cliquer sur le message ou en bas pour rÃ©duire/agrandir
4. **Changer de modÃ¨le** : Utiliser le sÃ©lecteur en haut du chat

## ğŸ”§ FonctionnalitÃ©s Techniques Actuelles

### Analyse Contextuelle Intelligente
L'extension analyse automatiquement :
- **Imports locaux** : DÃ©tection des fichiers liÃ©s au code sÃ©lectionnÃ©
- **DÃ©pendances externes** : Identification des bibliothÃ¨ques utilisÃ©es
- **Structure projet** : ComprÃ©hension de l'organisation du code
- **Code pertinent** : Inclusion du contexte nÃ©cessaire Ã  la comprÃ©hension

### Interface Chat AvancÃ©e
- **Rendu Markdown** : Support complet avec `marked.js`
- **Coloration syntaxique** : Highlight.js pour les blocs de code
- **Messages compressibles** : RÃ©duction/expansion des rÃ©ponses longues
- **Historique persistant** : Sauvegarde entre les sessions

### SystÃ¨me de Configuration
- **URL serveur** : Configuration flexible du serveur Ollama
- **SÃ©lection modÃ¨le** : Liste dynamique des modÃ¨les disponibles
- **Mode contexte** : Activation/dÃ©sactivation de l'analyse complÃ¨te
- **PrÃ©visualisation** : Option pour valider avant envoi

## ğŸ“ Architecture du Code Actuel

### Structure Simple (Version Actuelle)
```
src/
â”œâ”€â”€ extension.ts        # 1515 lignes - Point d'entrÃ©e avec toute la logique
â”œâ”€â”€ chatProvider.ts     # Interface WebView pour le chat
â”œâ”€â”€ ollamaService.ts    # Service API pour communiquer avec Ollama
â””â”€â”€ test/              # Tests unitaires
```

### DÃ©fis IdentifiÃ©s
- **Fichier monolithique** : `extension.ts` devient trop long (1515 lignes)
- **Logique mÃ©langÃ©e** : Analyse, interface, et service dans un mÃªme fichier
- **Maintenance difficile** : ComplexitÃ© croissante du code
- **ExtensibilitÃ© limitÃ©e** : Ajout de nouvelles fonctionnalitÃ©s complexe

## ğŸ¯ Roadmap de DÃ©veloppement

### Ã‰tape 1 : Modularisation (PrioritÃ©)
**ProblÃ¨me** : "le fichier devient trop long"
- [ ] Extraction du moteur d'analyse vers `core/analysis/`
- [ ] SÃ©paration de la gestion du contexte vers `core/context/`
- [ ] Utilitaires partagÃ©s vers `utils/`
- [ ] Interfaces et types vers `interfaces/`

### Ã‰tape 2 : Contexte Persistant
**Objectif** : "le contexte obtenu doit Ãªtre stockÃ© et structurÃ© pour premiÃ¨rement servir de 'sauvegarde' mais aussi pour Ãªtre utilisÃ© par d'autres LLM"
- [ ] SystÃ¨me de sauvegarde `.ollama-context/`
- [ ] Format JSON structurÃ© et rÃ©utilisable
- [ ] Export pour OpenAI, Anthropic, autres LLMs
- [ ] Restauration automatique du contexte

### Ã‰tape 3 : Intelligence AvancÃ©e
- [ ] DÃ©tection automatique de frameworks (React, Vue, Angular, etc.)
- [ ] GÃ©nÃ©ration de ModelFiles spÃ©cialisÃ©s selon le projet
- [ ] Recommandation automatique de LLM selon les capacitÃ©s machine
- [ ] Optimisation des paramÃ¨tres (context_length, num_ctx) selon la RAM
- [ ] Fallback intelligent vers des modÃ¨les plus lÃ©gers

### Ã‰tape 4 : Optimisation
- [ ] Interface utilisateur amÃ©liorÃ©e
- [ ] Performance et scalabilitÃ©
- [ ] Support multi-workspace
- [ ] IntÃ©gration avec autres outils de dÃ©veloppement

### Ã‰tape 5 : Support Multi-Langues
- [ ] Interface utilisateur multilingue (franÃ§ais, anglais,...)
- [ ] DÃ©tection automatique de la langue du systÃ¨me
- [ ] Configuration de langue personnalisÃ©e
- [ ] Support pour les commentaires de code en plusieurs langues
- [ ] Adaptation des prompts selon la langue sÃ©lectionnÃ©e
- [ ] Documentation multilingue
- [ ] Messages du chat adaptÃ©s Ã  la langue prÃ©fÃ©rÃ©e


## ğŸ’¡ Vision Future : Contexte Multi-LLM

### Concept Central
**"Le contexte obtenu doit Ãªtre stockÃ© et structurÃ© pour premiÃ¨rement servir de 'sauvegarde' mais aussi pour Ãªtre utilisÃ© par d'autres LLM"**

### Structure de Contexte EnvisagÃ©e
```json
{
  "metadata": {
    "project_type": "web|mobile|desktop|library",
    "framework": "react|vue|angular|express|...",
    "language": "typescript|javascript|python|...",
    "analysis_date": "2024-01-15T10:30:00Z"
  },
  "dependencies": {
    "local": [
      {
        "path": "./src/components/Button.tsx",
        "imports": ["React", "./types"],
        "exports": ["Button", "ButtonProps"]
      }
    ],
    "external": [
      {
        "name": "react",
        "version": "18.2.0",
        "usage": "critical"
      }
    ]
  },
  "structure": {
    "components": ["Button", "Header", "Layout"],
    "services": ["api", "auth", "storage"],
    "utils": ["helpers", "constants"]
  },
  "exports": {
    "ollama": "FROM codellama:7b\\nSYSTEM Tu es un expert React...",
    "openai": {
      "system_prompt": "Tu es un assistant spÃ©cialisÃ© en React...",
      "context": {...}
    },
    "anthropic": {
      "system": "Vous Ãªtes un expert en dÃ©veloppement React...",
      "context": {...}
    }
  }
}
```

### Utilisation Multi-LLM
```bash
# Export pour OpenAI
curl -X POST https://api.openai.com/v1/chat/completions \\
  -d @.ollama-context/openai-export.json

# Export pour Claude
curl -X POST https://api.anthropic.com/v1/messages \\
  -d @.ollama-context/anthropic-export.json

# ModelFile pour Ollama
ollama create my-project-expert -f .ollama-context/modelfile
```

## ğŸ¤ Contribution

### Ã‰tat Actuel
- âœ… Interface chat fonctionnelle avec boutons rÃ©duire
- âœ… Analyse contextuelle de base
- âœ… IntÃ©gration Ollama complÃ¨te
- âœ… Configuration flexible

### Prochaines Contributions SouhaitÃ©es
- ğŸ”„ Modularisation du code (fichier trop long)
- ğŸ’¾ ImplÃ©mentation du systÃ¨me de contexte persistant
- ğŸ¤– GÃ©nÃ©ration automatique de ModelFiles dÃ©pendament du context du projet 
- ğŸ“Š RÃ©ponses adaptatives selon la complexitÃ©

---

## ğŸ“ Support et Feedback

Cette extension Ã©volue selon les besoins rÃ©els d'utilisation. N'hÃ©sitez pas Ã  :
- Reporter des bugs ou problÃ¨mes rencontrÃ©s
- SuggÃ©rer des amÃ©liorations basÃ©es sur votre workflow
- Partager des exemples d'usage avec diffÃ©rents types de projets
- Contribuer au dÃ©veloppement des fonctionnalitÃ©s planifiÃ©es

**L'objectif est de crÃ©er l'extension d'assistant IA local la plus avancÃ©e possible, offrant toutes les fonctionnalitÃ©s des assistants de code existants tout en restant entiÃ¨rement contextuelle et rÃ©utilisable ! ğŸš€**

### ğŸ¯ Vision : Assistant IA Local Complet

Cette extension vise Ã  reproduire et amÃ©liorer les capacitÃ©s des assistants IA populaires :

#### ğŸ”¥ FonctionnalitÃ©s d'Assistant de Code VisÃ©es
- **AutocomplÃ©tion intelligente** : Suggestions de code en temps rÃ©el
- **GÃ©nÃ©ration de code** : CrÃ©ation de fonctions, classes et modules complets  
- **Refactoring assistÃ©** : AmÃ©lioration et restructuration automatique
- **Documentation automatique** : GÃ©nÃ©ration de commentaires et docs
- **Tests unitaires** : CrÃ©ation automatique de tests basÃ©s sur le code
- **Correction d'erreurs** : DÃ©tection et suggestions de correction
- **Explication de code** : Analyse et explications dÃ©taillÃ©es
- **Conversion de langages** : Translation entre diffÃ©rents langages de programmation

#### ğŸ  Avantages du Local
- **ConfidentialitÃ© totale** : Aucune donnÃ©e envoyÃ©e vers des serveurs externes
- **Personnalisation** : ModÃ¨les adaptÃ©s spÃ©cifiquement Ã  votre projet
- **Performance** : Latence minimale avec Ollama local
- **CoÃ»t zÃ©ro** : Pas d'abonnement ou de tokens payants
- **DisponibilitÃ©** : Fonctionne mÃªme hors ligne

#### ğŸ”„ ContextualitÃ© et RÃ©utilisabilitÃ©
- **Apprentissage continu** : Le contexte s'enrichit Ã  chaque utilisation
- **MÃ©moire de projet** : L'assistant "connaÃ®t" votre codebase
- **Patterns personnalisÃ©s** : DÃ©tection de vos conventions de codage
- **Export universel** : Contexte rÃ©utilisable avec d'autres LLMs
- **Ã‰volution adaptative** : L'assistant s'amÃ©liore avec le projet
