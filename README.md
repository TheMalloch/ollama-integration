### package.json
```json
{
  "name": "ollama-chat",
  "displayName": "Ollama Chat",
  "description": "Chat with LLMs like Ollama directly from VSCode.",
  "version": "0.1.0",
  "publisher": "your-publisher-name",
  "engines": {
    "vscode": "^1.70.0"
  },
  "activationEvents": [
    "onCommand:ollamaChat.open"
  ],
  "main": "./out/extension.js",
  "contributes": {
    "commands": [
      {
        "command": "ollamaChat.open",
        "title": "Open Ollama Chat"
      }
    ]
  },
  "scripts": {
    "vscode:prepublish": "npm run compile",
    "compile": "tsc -p ./",
    "watch": "tsc -watch -p ./",
    "postinstall": "node ./node_modules/vscode/bin/install"
  },
  "devDependencies": {
    "@types/vscode": "^1.70.0",
    "@types/node": "^18.0.0",
    "typescript": "^4.6.2",
    "vscode-test": "^1.5.0"
  }
}
```

### src/extension.ts
```typescript
import * as vscode from 'vscode';
import { OllamaChatProvider } from './ollamaChatProvider';

export function activate(context: vscode.ExtensionContext) {
  const provider = new OllamaChatProvider(context);
  context.subscriptions.push(
    vscode.commands.registerCommand('ollamaChat.open', () => provider.show())
  );
}

export function deactivate() {}
```

### src/ollamaChatProvider.ts
```typescript
import * as vscode from 'vscode';
import { WebviewPanel, WebviewOptions, Uri } from 'vscode';

export class OllamaChatProvider {
  private panel: WebviewPanel | undefined;

  constructor(private readonly context: vscode.ExtensionContext) {}

  public show() {
    if (this.panel) {
      this.panel.reveal();
      return;
    }

    const column = vscode.window.activeTextEditor ? vscode.ViewColumn.Beside : vscode.ViewColumn.One;
    this.panel = vscode.window.createWebviewPanel(
      'ollamaChat',
      'Ollama Chat',
      column,
      {
        enableScripts: true,
        localResourceRoots: [Uri.joinPath(this.context.extensionUri, 'out')]
      }
    );

    this.panel.onDidDispose(() => {
      this.panel = undefined;
    });

    this.panel.webview.html = this.getWebviewContent();
  }

  private getWebviewContent() {
    const scriptUri = this.panel!.webview.asWebviewUri(Uri.joinPath(this.context.extensionUri, 'out', 'ollamaChat.js'));
    return `
      <!DOCTYPE html>
      <html lang="en">
        <head>
          <meta charset="UTF-8">
          <meta name="viewport" content="width=device-width, initial-scale=1.0">
          <title>Ollama Chat</title>
        </head>
        <body>
          <div id="app"></div>
          <script src="${scriptUri}"></script>
        </body>
      </html>
    `;
  }
}
```

### out/ollamaChat.js
```javascript
document.addEventListener('DOMContentLoaded', () => {
  const app = document.getElementById('app');
  const chatInput = document.createElement('input');
  const sendButton = document.createElement('button');
  sendButton.textContent = 'Send';
  app.appendChild(chatInput);
  app.appendChild(sendButton);

  let messages = [];

  sendButton.addEventListener('click', async () => {
    const message = chatInput.value.trim();
    if (message) {
      appendMessage('user', message);
      chatInput.value = '';

      try {
        const response = await fetch('/api/generate', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ text: message })
        });

        if (!response.ok) {
          throw new Error('API request failed');
        }

        const data = await response.json();
        appendMessage('assistant', data.text);
      } catch (error) {
        console.error(error);
        appendMessage('error', 'Failed to get response from Ollama');
      }
    }
  });

  function appendMessage(sender, text) {
    const messageElement = document.createElement('div');
    messageElement.className = `message ${sender}`;
    messageElement.textContent = text;
    app.appendChild(messageElement);
  }
});
```

### out/ollamaChat.css
```css
body {
  font-family: Arial, sans-serif;
}

.message {
  margin: 10px 0;
}

.user {
  color: #3498db;
}

.assistant {
  color: #2ecc71;
}

.error {
  color: #e74c3c;
}
```

# Ollama Integration Extension

Extension VS Code avanc√©e pour l'int√©gration intelligente avec Ollama et autres LLMs. Cette extension offre une analyse contextuelle approfondie du code et une interaction intelligente avec les mod√®les de langage.

## ‚ú® Fonctionnalit√©s Principales

### üß† Analyse Contextuelle Intelligente
- **Analyse des d√©pendances** : D√©tection et analyse automatique des imports locaux
- **Compr√©hension du projet** : Reconnaissance des frameworks et types de projets
- **Structure du codebase** : Analyse de l'architecture et des patterns utilis√©s
- **Messages compressibles** : R√©duction/expansion des r√©ponses longues

### üíæ Syst√®me de Sauvegarde Contextuelle
- **Stockage persistant** : Le contexte analys√© est automatiquement sauvegard√©
- **Structure r√©utilisable** : Donn√©es format√©es pour √™tre utilis√©es par d'autres LLMs
- **Historique intelligent** : Conservation des analyses pr√©c√©dentes pour am√©liorer les futures interactions
- **Export/Import** : Possibilit√© d'exporter le contexte pour d'autres outils

### üéØ R√©ponses Structur√©es et Adapt√©es
- **Format intelligent** : Les r√©ponses sont format√©es selon le type de contenu (code, documentation, analyse)
- **Limite adaptative** : La taille des r√©ponses s'adapte √† la complexit√© du codebase
- **Pr√©sentation code** : Mise en forme optimis√©e pour les extraits de code
- **Validation contextuelle** : V√©rification que les r√©ponses correspondent au contexte du projet

### üîç Compr√©hension Avanc√©e des Projets
- **D√©tection de framework** : Reconnaissance automatique des frameworks utilis√©s (React, Vue, Angular, etc.)
- **Type de projet** : Identification du type d'application (web, desktop, mobile, library, etc.)
- **Patterns architecturaux** : D√©tection des patterns (MVC, MVP, hexagonal, etc.)
- **Recommandations cibl√©es** : Conseils adapt√©s au type de projet d√©tect√©

### ü§ñ G√©n√©ration de ModelFiles Sp√©cialis√©s
- **LLM sp√©cialis√©** : Utilisation d'un mod√®le l√©ger pour g√©n√©rer des ModelFiles Ollama personnalis√©s
- **Context-aware** : ModelFiles g√©n√©r√©s en fonction du contexte du projet
- **Prompts stricts** : Cr√©ation de prompts sp√©cialis√©s pour des r√©ponses plus pr√©cises
- **Templates adaptatifs** : G√©n√©ration de templates selon le framework d√©tect√©

## üöÄ Installation

1. Ouvrez VS Code
2. Allez dans les Extensions (`Ctrl+Shift+X`)
3. Recherchez "Ollama Integration"
4. Cliquez sur "Installer"

## üìñ Utilisation

### Analyse Rapide
1. Ouvrez un fichier de code
2. S√©lectionnez du code (optionnel)
3. Utilisez `Ctrl+Shift+P` ‚Üí "Ollama: Send to Chat"
4. Choisissez votre type d'analyse

### Configuration Avanc√©e
- **Mode Smart** : Analyse compl√®te avec d√©pendances (`ollama.useFullContext: true`)
- **Mode Basic** : Analyse simple du code s√©lectionn√© (`ollama.useFullContext: false`)
- **Pr√©visualisation** : Voir le message avant envoi (`ollama.showPreviewBeforeSending: true`)

## üèóÔ∏è Architecture Modulaire

Pour maintenir une base de code propre, l'extension est organis√©e en modules :

```
src/
‚îú‚îÄ‚îÄ extension.ts           # Point d'entr√©e principal
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ analysis/         # Moteur d'analyse du code
‚îÇ   ‚îú‚îÄ‚îÄ context/          # Gestion du contexte et sauvegarde
‚îÇ   ‚îú‚îÄ‚îÄ llm/             # Int√©gration LLM et ModelFile
‚îÇ   ‚îî‚îÄ‚îÄ project/         # D√©tection de projet et framework
‚îú‚îÄ‚îÄ providers/
‚îÇ   ‚îú‚îÄ‚îÄ chatProvider.ts   # Interface de chat
‚îÇ   ‚îî‚îÄ‚îÄ webview/         # Composants webview
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ fileUtils.ts     # Utilitaires fichiers
    ‚îî‚îÄ‚îÄ projectDetector.ts # D√©tection de projets
```

## üîß Fonctionnalit√©s Techniques

### Stockage Contextuel
```typescript
interface ProjectContext {
  metadata: ProjectMetadata;
  dependencies: DependencyGraph;
  structure: CodeStructure;
  frameworks: FrameworkInfo[];
  modelFiles: GeneratedModelFiles;
  history: AnalysisHistory[];
}
```

### D√©tection de Projet
- **Framework detection** : Analyse des `package.json`, `composer.json`, `requirements.txt`, etc.
- **Project patterns** : Reconnaissance des structures de dossiers typiques
- **Technology stack** : Identification des technologies utilis√©es
- **Code patterns** : Analyse des patterns de code utilis√©s

### G√©n√©ration ModelFile
```ollama
# Exemple de ModelFile g√©n√©r√© pour un projet React
FROM llama3.2:1b

PARAMETER temperature 0.3
PARAMETER top_p 0.9

SYSTEM """
Tu es un expert React/TypeScript sp√©cialis√© dans ce projet.
Context: Application React avec TypeScript, utilisant Hooks et Context API.
Framework: React 18.x avec Vite
Patterns d√©tect√©s: Component composition, Custom hooks, State management local

R√©ponds uniquement avec du code React/TypeScript valide et des explications concises.
Utilise les patterns d√©tect√©s dans le projet.
"""
```

## ‚öôÔ∏è Configuration

### Param√®tres Principaux
```json
{
  "ollama.useFullContext": true,
  "ollama.showPreviewBeforeSending": false,
  "ollama.contextStorage.enabled": true,
  "ollama.contextStorage.path": "./ollama-context",
  "ollama.projectDetection.enabled": true,
  "ollama.modelFile.autoGenerate": true,
  "ollama.response.maxTokens": "adaptive",
  "ollama.response.format": "structured"
}
```

### Sauvegarde Contextuelle
- **Emplacement** : `.ollama-context/` dans le workspace
- **Format** : JSON structur√© avec m√©tadonn√©es
- **Compression** : Compression automatique des gros contextes
- **Versioning** : Suivi des versions du contexte

## üéØ Roadmap

### Phase 1 - Architecture (En cours)
- [x] Analyse contextuelle de base
- [x] Interface chat avec compression
- [ ] Modularisation du code
- [ ] Syst√®me de sauvegarde contextuelle

### Phase 2 - Intelligence (Prochaine)
- [ ] D√©tection avanc√©e de frameworks
- [ ] G√©n√©ration de ModelFiles sp√©cialis√©s
- [ ] R√©ponses structur√©es et adaptatives
- [ ] Cache intelligent du contexte

### Phase 3 - Optimisation
- [ ] Performance et scalabilit√©
- [ ] Support multi-LLM
- [ ] Int√©gration CI/CD
- [ ] Plugins tiers

## ü§ù Contribution

1. Fork le projet
2. Cr√©ez une branche pour votre fonctionnalit√©
3. Committez vos changements
4. Poussez vers la branche
5. Ouvrez une Pull Request

## üìù Notes Techniques

### Gestion de la Complexit√©
- **Limite adaptive** : La taille des r√©ponses s'adapte automatiquement √† la taille du codebase
- **Context chunking** : Division intelligente du contexte pour les gros projets
- **Selective analysis** : Analyse cibl√©e selon le type de requ√™te

### Performance
- **Lazy loading** : Chargement √† la demande des d√©pendances
- **Cache strat√©gique** : Mise en cache des analyses fr√©quentes
- **Background processing** : Traitement en arri√®re-plan pour les analyses lourdes
