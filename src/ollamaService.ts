import axios from 'axios';
import * as vscode from 'vscode';

interface ChatMessage {
    role: 'user' | 'assistant';
    content: string;
}
interface OllamaModel {
    name: string;
    modified_at: string;
    size: number;
}


export class OllamaService {
    private getConfig() {
        const config = vscode.workspace.getConfiguration('ollama');
        return {
            serverUrl: config.get('serverUrl', 'http://localhost:11434'),
            model: config.get('model', ''), // Pas de mod√®le par d√©faut, sera auto-d√©tect√©
            disableThinking: config.get('disableThinking', true) // Par d√©faut, d√©sactiver le "thinking"
        };
    }

    // Helper pour construire les options avec le contr√¥le du "thinking"
    private buildRequestOptions(baseOptions: any): any {
        const config = this.getConfig();
        const options = { ...baseOptions };
        
        if (config.disableThinking) {
            options.no_thinking = true;
        }
        
        return options;
    }
    
    // Helper pour savoir si le no_thinking est activ√©
    private isThinkingDisabled(): boolean {
        return this.getConfig().disableThinking;
    }

    // Helper pour debug optionnel du streaming
    private debugChunk(chunk: string, context: string): void {
        const config = vscode.workspace.getConfiguration('ollama');
        const debugMode = config.get('debugStreaming', false);
        
        if (debugMode) {
            console.log(`[${context}] Chunk re√ßu (${chunk.length} chars):`, chunk.substring(0, 100));
            
            // V√©rifier si c'est du JSON valide
            try {
                JSON.parse(chunk);
                console.log(`[${context}] ‚úÖ JSON valide`);
            } catch {
                console.log(`[${context}] ‚ùå JSON invalide`);
            }
        }
    }

    // NOUVELLE M√âTHODE: Obtenir automatiquement un mod√®le disponible
    async getAutoSelectedModel(): Promise<string> {
        try {
            const models = await this.getModels();
            
            if (models.length === 0) {
                throw new Error('Aucun mod√®le Ollama install√©. Suggestions:\n' +
                    '- ollama pull qwen2.5-coder:7b (sp√©cialis√© code)\n' +
                    '- ollama pull llama3.2:3b (l√©ger et rapide)\n' +
                    '- ollama pull phi3:mini (tr√®s l√©ger)');
            }
            
            // Ordre de pr√©f√©rence pour les mod√®les (m√™me logique que l'orchestrateur)
            const preferenceOrder = [
                'qwen2.5-coder:32b', 'qwen2.5-coder:14b', 'qwen2.5-coder:7b', 'qwen2.5-coder:3b',
                'codellama:34b', 'codellama:13b', 'codellama:7b', 'codellama:7b-code',
                'deepseek-coder:33b', 'deepseek-coder:6.7b', 'deepseek-coder:1.3b',
                'llama3.1:70b', 'llama3.1:8b', 'llama3.2:3b', 'llama3.2:1b',
                'gemma2:27b', 'gemma2:9b', 'gemma2:2b', 'mistral:7b', 'phi3:14b', 'phi3:mini'
            ];
            
            // Chercher le premier mod√®le disponible selon l'ordre de pr√©f√©rence
            for (const preferredModel of preferenceOrder) {
                const found = models.find(m => 
                    m.name === preferredModel || 
                    m.name.startsWith(preferredModel.split(':')[0])
                );
                
                if (found) {
                    console.log(`Mod√®le auto-s√©lectionn√©: ${found.name}`);
                    return found.name;
                }
            }
            
            // Si aucun mod√®le pr√©f√©r√© n'est trouv√©, prendre le premier disponible
            const fallbackModel = models[0].name;
            console.log(`Aucun mod√®le pr√©f√©r√© trouv√©, utilisation de: ${fallbackModel}`);
            return fallbackModel;
        } catch (error) {
            throw new Error(`D√©tection mod√®le √©chou√©e: ${error}`);
        }
    }

    async chat(message: string, history: ChatMessage[]): Promise<string> {
        const config = this.getConfig();
        
        // Si pas de mod√®le configur√©, d√©tecter automatiquement
        let modelToUse = config.model;
        if (!modelToUse) {
            console.log('Aucun mod√®le sp√©cifi√©, auto-s√©lection...');
            try {
                modelToUse = await this.getAutoSelectedModel();
            } catch (error) {
                throw error;
            }
        } else {
            // V√©rifier que le mod√®le configur√© existe
            try {
                const models = await this.getModels();
                const modelExists = models.some(m => m.name === modelToUse);
                
                if (!modelExists) {
                    console.warn(`Mod√®le ${modelToUse} introuvable, recherche d'une alternative...`);
                    modelToUse = await this.getAutoSelectedModel();
                }
            } catch (error) {
                console.warn('Impossible de v√©rifier les mod√®les, tentative avec le mod√®le configur√©...');
            }
        }

        console.log(`Utilisation du mod√®le: ${modelToUse}`);
        
        try {
            // Construire le contexte de conversation
            let conversation = '';
            if (history.length > 1) {
                // Prendre les derniers messages (limiter pour √©viter les tokens excessifs)
                const recentHistory = history.slice(-10);
                conversation = recentHistory
                    .slice(0, -1) // Exclure le dernier message (current)
                    .map(msg => `${msg.role === 'user' ? 'Utilisateur' : 'Assistant'}: ${msg.content}`)
                    .join('\n\n');
                conversation += '\n\n';
            }

            const fullPrompt = conversation + `Utilisateur: ${message}\nAssistant:`;

            const response = await axios.post(`${config.serverUrl}/api/generate`, {
                model: modelToUse,
                prompt: fullPrompt,
                stream: false,
                options: this.buildRequestOptions({
                    temperature: 0.7,
                    num_predict: 1000
                })
            }, {
                timeout: 180000 // Augment√© pour les mod√®les lourds
            });

            if (response.data?.response) {
                return response.data.response.trim();
            } else {
                throw new Error('R√©ponse invalide d\'Ollama');
            }

        } catch (error) {
            if (axios.isAxiosError(error)) {
                if (error.code === 'ECONNREFUSED') {
                    throw new Error(`Impossible de se connecter √† Ollama sur ${config.serverUrl}`);
                } else if (error.response?.status === 404) {
                    throw new Error(`Mod√®le '${modelToUse}' introuvable`);
                } else {
                    throw new Error(`Erreur API Ollama: ${error.message}`);
                }
            } else {
                throw new Error(`Erreur inattendue: ${error}`);
            }
        }
    }

    // üöÄ NOUVELLE M√âTHODE: Chat avec streaming token par token
    async chatStream(
        message: string, 
        history: ChatMessage[],
        onToken: (token: string) => void,
        onComplete: (fullResponse: string) => void,
        onError: (error: string) => void
    ): Promise<void> {
        const config = this.getConfig();
        
        // Auto-s√©lection du mod√®le (m√™me logique que chat())
        let modelToUse = config.model;
        if (!modelToUse) {
            console.log('Aucun mod√®le sp√©cifi√© pour streaming, auto-s√©lection...');
            try {
                modelToUse = await this.getAutoSelectedModel();
            } catch (error) {
                onError(`D√©tection mod√®le √©chou√©e: ${error}`);
                return;
            }
        } else {
            try {
                const models = await this.getModels();
                const modelExists = models.some(m => m.name === modelToUse);
                
                if (!modelExists) {
                    console.warn(`Mod√®le ${modelToUse} introuvable pour streaming, recherche d'une alternative...`);
                    modelToUse = await this.getAutoSelectedModel();
                }
            } catch (error) {
                console.warn('Impossible de v√©rifier les mod√®les pour streaming, tentative avec le mod√®le configur√©...');
            }
        }

        console.log(`Utilisation du mod√®le pour streaming: ${modelToUse}`);
        
        try {
            // Construire le contexte de conversation
            let conversation = '';
            if (history.length > 1) {
                const recentHistory = history.slice(-10);
                conversation = recentHistory
                    .slice(0, -1)
                    .map(msg => `${msg.role === 'user' ? 'Utilisateur' : 'Assistant'}: ${msg.content}`)
                    .join('\n\n');
                conversation += '\n\n';
            }

            const fullPrompt = conversation + `Utilisateur: ${message}\nAssistant:`;

            const response = await axios.post(`${config.serverUrl}/api/generate`, {
                model: modelToUse,
                prompt: fullPrompt,
                stream: true, // Activer le streaming
                options: this.buildRequestOptions({
                    temperature: 0.7,
                    num_predict: 1000
                })
            }, {
                timeout: 180000,
                responseType: 'stream'
            });

            let fullResponse = '';
            let buffer = ''; // Buffer pour accumuler les chunks partiels
            
            response.data.on('data', (chunk: Buffer) => {
                try {
                    // Convertir le chunk en string et l'ajouter au buffer
                    buffer += chunk.toString();
                    
                    // S√©parer les lignes compl√®tes
                    const lines = buffer.split('\n');
                    
                    // Garder la derni√®re ligne (potentiellement incompl√®te) dans le buffer
                    buffer = lines.pop() || '';
                    
                    // Traiter chaque ligne compl√®te
                    lines.forEach(line => {
                        const trimmedLine = line.trim();
                        if (trimmedLine) {
                            try {
                                const data = JSON.parse(trimmedLine);
                                
                                if (data.response) {
                                    fullResponse += data.response;
                                    onToken(data.response); // üöÄ Callback token par token
                                }
                                
                                if (data.done) {
                                    onComplete(fullResponse.trim()); // üéØ Callback compl√©tion
                                }
                            } catch (parseError) {
                                // Ignorer silencieusement les lignes non-JSON ou mal form√©es
                                // console.debug(`Ligne ignor√©e (non-JSON): ${trimmedLine.substring(0, 50)}...`);
                            }
                        }
                    });
                } catch (error) {
                    console.warn('Erreur traitement chunk streaming:', error);
                    // Ne pas interrompre le streaming pour une erreur de chunk
                }
            });

            response.data.on('end', () => {
                // Traiter le buffer restant s'il y en a un
                if (buffer.trim()) {
                    try {
                        const data = JSON.parse(buffer.trim());
                        if (data.response) {
                            fullResponse += data.response;
                            onToken(data.response);
                        }
                        if (data.done) {
                            onComplete(fullResponse.trim());
                        }
                    } catch (error) {
                        // Buffer final ignor√© s'il n'est pas du JSON valide
                    }
                }
                
                // S'assurer que onComplete est appel√© m√™me si pas de 'done' re√ßu
                if (fullResponse && !fullResponse.includes('undefined')) {
                    onComplete(fullResponse.trim());
                }
            });

            response.data.on('error', (error: any) => {
                onError(`Erreur stream: ${error.message}`);
            });

        } catch (error) {
            if (axios.isAxiosError(error)) {
                if (error.code === 'ECONNREFUSED') {
                    onError(`Impossible de se connecter √† Ollama sur ${config.serverUrl}`);
                } else if (error.response?.status === 404) {
                    onError(`Mod√®le '${modelToUse}' introuvable`);
                } else {
                    onError(`Erreur API Ollama: ${error.message}`);
                }
            } else {
                onError(`Erreur inattendue: ${error}`);
            }
        }
    }

    // NOUVELLE M√âTHODE: Chat streaming pour l'orchestrateur
    async chatStreamForWorker(
        prompt: string,
        onToken: (token: string) => void,
        onComplete: (fullResponse: string) => void
    ): Promise<void> {
        const config = this.getConfig();
        
        // Utiliser exactement la m√™me logique que chat() pour √©viter les erreurs 404
        let modelToUse = config.model;
        if (!modelToUse) {
            console.log('Aucun mod√®le sp√©cifi√© pour worker, auto-s√©lection...');
            try {
                modelToUse = await this.getAutoSelectedModel();
            } catch (error) {
                throw new Error(`D√©tection mod√®le √©chou√©e pour worker: ${error}`);
            }
        } else {
            // V√©rifier que le mod√®le configur√© existe (m√™me logique que chat())
            try {
                const models = await this.getModels();
                const modelExists = models.some(m => m.name === modelToUse);
                
                if (!modelExists) {
                    console.warn(`Mod√®le ${modelToUse} introuvable pour worker, recherche d'une alternative...`);
                    modelToUse = await this.getAutoSelectedModel();
                }
            } catch (error) {
                console.warn('Impossible de v√©rifier les mod√®les pour worker, tentative avec le mod√®le configur√©...');
            }
        }

        console.log(`Worker utilise le mod√®le: ${modelToUse}`);
        
        try {
            const response = await axios.post(`${config.serverUrl}/api/generate`, {
                model: modelToUse,
                prompt: prompt,
                stream: true,
                options: this.buildRequestOptions({
                    temperature: 0.2, // Plus bas pour consistance
                    num_predict: 500,
                    num_ctx: 2048
                })
            }, {
                timeout: 90000,
                responseType: 'stream'
            });

            let fullResponse = '';
            let buffer = ''; // Buffer pour accumuler les chunks partiels
            
            response.data.on('data', (chunk: Buffer) => {
                try {
                    // Convertir le chunk en string et l'ajouter au buffer
                    buffer += chunk.toString();
                    
                    // S√©parer les lignes compl√®tes
                    const lines = buffer.split('\n');
                    
                    // Garder la derni√®re ligne (potentiellement incompl√®te) dans le buffer
                    buffer = lines.pop() || '';
                    
                    // Traiter chaque ligne compl√®te
                    lines.forEach(line => {
                        const trimmedLine = line.trim();
                        if (trimmedLine) {
                            try {
                                const data = JSON.parse(trimmedLine);
                                
                                if (data.response) {
                                    fullResponse += data.response;
                                    onToken(data.response);
                                }
                                
                                if (data.done) {
                                    onComplete(fullResponse.trim());
                                }
                            } catch (parseError) {
                                // Ignorer silencieusement les erreurs de parsing JSON
                                // console.debug(`Worker chunk ignor√©: ${trimmedLine.substring(0, 30)}...`);
                            }
                        }
                    });
                } catch (error) {
                    console.warn('Erreur worker streaming chunk:', error);
                    // Ne pas interrompre le streaming pour une erreur de chunk
                }
            });

            response.data.on('end', () => {
                // Traiter le buffer restant s'il y en a un
                if (buffer.trim()) {
                    try {
                        const data = JSON.parse(buffer.trim());
                        if (data.response) {
                            fullResponse += data.response;
                            onToken(data.response);
                        }
                        if (data.done) {
                            onComplete(fullResponse.trim());
                        }
                    } catch (error) {
                        // Buffer final ignor√© s'il n'est pas du JSON valide
                    }
                }
                
                // S'assurer que onComplete est appel√©
                if (fullResponse) {
                    onComplete(fullResponse.trim());
                }
            });

            response.data.on('error', (error: any) => {
                console.error('Erreur worker stream:', error);
                onComplete(fullResponse || 'Erreur streaming worker');
            });

        } catch (error) {
            throw new Error(`Erreur streaming: ${error}`);
        }
    }

    async testConnection(): Promise<boolean> {
        const config = this.getConfig();
        try {
            await axios.get(`${config.serverUrl}/api/tags`, { timeout: 5000 });
            return true;
        } catch {
            return false;
        }
    }
    async getModels(): Promise<OllamaModel[]> {
        const config = this.getConfig();
        try {
            const response = await axios.get(`${config.serverUrl}/api/tags`, {
                timeout: 10000
            });
            
            return response.data?.models || [];
        } catch (error) {
            if (axios.isAxiosError(error)) {
                if (error.code === 'ECONNREFUSED') {
                    throw new Error(`Impossible de se connecter √† Ollama sur ${config.serverUrl}`);
                } else {
                    throw new Error(`Erreur lors de la r√©cup√©ration des mod√®les: ${error.message}`);
                }
            } else {
                throw new Error(`Erreur inattendue: ${error}`);
            }
        }
    }

    // =====================================================
    // SYST√àME DE TOOLS/ACTIONS POUR LE LLM
    // =====================================================

    // D√©finition des tools disponibles pour le LLM
    private getAvailableTools(): any[] {
        return [
            {
                type: "function",
                function: {
                    name: "check_workspace",
                    description: "Analyse l'espace de travail actuel pour d√©tecter le type de projet, frameworks, et structure",
                    parameters: {
                        type: "object",
                        properties: {},
                        required: []
                    }
                }
            },
            {
                type: "function", 
                function: {
                    name: "check_directory",
                    description: "Explore un r√©pertoire sp√©cifique et liste son contenu",
                    parameters: {
                        type: "object",
                        properties: {
                            path: {
                                type: "string",
                                description: "Chemin du r√©pertoire √† explorer (relatif au workspace)"
                            }
                        },
                        required: ["path"]
                    }
                }
            },
            {
                type: "function",
                function: {
                    name: "read_file",
                    description: "Lit et analyse le contenu d'un fichier sp√©cifique",
                    parameters: {
                        type: "object",
                        properties: {
                            filepath: {
                                type: "string",
                                description: "Chemin du fichier √† lire"
                            }
                        },
                        required: ["filepath"]
                    }
                }
            },
            {
                type: "function",
                function: {
                    name: "find_functions",
                    description: "Trouve toutes les fonctions dans un r√©pertoire ou fichier",
                    parameters: {
                        type: "object",
                        properties: {
                            target: {
                                type: "string",
                                description: "Chemin du fichier ou r√©pertoire √† analyser"
                            },
                            pattern: {
                                type: "string",
                                description: "Pattern de fichiers √† inclure (ex: *.ts, *.js)",
                                default: "*"
                            }
                        },
                        required: ["target"]
                    }
                }
            },
            {
                type: "function",
                function: {
                    name: "check_dependencies",
                    description: "Analyse les d√©pendances du projet (package.json, imports)",
                    parameters: {
                        type: "object",
                        properties: {},
                        required: []
                    }
                }
            },
            {
                type: "function",
                function: {
                    name: "generate_code",
                    description: "G√©n√®re du code selon les sp√©cifications donn√©es",
                    parameters: {
                        type: "object",
                        properties: {
                            type: {
                                type: "string",
                                enum: ["function", "class", "component", "interface", "type"],
                                description: "Type de code √† g√©n√©rer"
                            },
                            name: {
                                type: "string",
                                description: "Nom de l'√©l√©ment √† g√©n√©rer"
                            },
                            specifications: {
                                type: "string",
                                description: "Sp√©cifications d√©taill√©es du code √† g√©n√©rer"
                            }
                        },
                        required: ["type", "name", "specifications"]
                    }
                }
            }
        ];
    }

    // Chat avec support des tools - version simplifi√©e pour l'extension
    async chatWithToolsSimple(message: string): Promise<AsyncIterable<any>> {
        const config = this.getConfig();
        
        let modelToUse = config.model;
        if (!modelToUse || modelToUse === '') {
            modelToUse = await this.getAutoSelectedModel();
        }

        console.log(`üõ†Ô∏è Chat avec tools utilisant le mod√®le: ${modelToUse}`);

        const url = `${config.serverUrl}/api/chat`;
        
        // D'abord tester si Ollama supporte les tools
        try {
            return await this.chatWithToolsRequest(url, modelToUse, message, config);
        } catch (error) {
            console.warn('‚ö†Ô∏è √âchec du chat avec tools, tentative sans tools...', error);
            // Fallback: chat normal sans tools mais avec analyse manuelle
            return await this.chatWithAnalysis(url, modelToUse, message, config);
        }
    }

    private async chatWithToolsRequest(url: string, model: string, message: string, config: any): Promise<AsyncIterable<any>> {
        const tools = this.getAvailableTools();
        
        const requestBody = {
            model: model,
            messages: [
                {
                    role: 'user',
                    content: message
                }
            ],
            tools: tools,
            stream: true,
            options: this.buildRequestOptions(config)
        };

        console.log('üîß Tools disponibles:', tools.length);
        console.log('üì§ Envoi requ√™te avec tools √† Ollama...');

        const response = await fetch(url, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(requestBody)
        });

        if (!response.ok) {
            const errorText = await response.text();
            console.error('‚ùå D√©tails de l\'erreur Ollama:', errorText);
            console.error('üì§ Corps de la requ√™te envoy√©e:', JSON.stringify(requestBody, null, 2));
            throw new Error(`Erreur HTTP: ${response.status} - ${response.statusText}. D√©tails: ${errorText}`);
        }

        if (!response.body) {
            throw new Error('Pas de body dans la r√©ponse');
        }

        return this.parseStreamResponse(response.body);
    }

    private async chatWithAnalysis(url: string, model: string, message: string, config: any): Promise<AsyncIterable<any>> {
        console.log('üîÑ Mode fallback: chat avec analyse manuelle...');
        
        // Enrichir le message avec du contexte de workspace
        const enrichedMessage = await this.enrichMessageWithWorkspaceContext(message);
        
        const requestBody = {
            model: model,
            messages: [
                {
                    role: 'user',
                    content: enrichedMessage
                }
            ],
            stream: true,
            options: this.buildRequestOptions(config)
        };

        const response = await fetch(url, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(requestBody)
        });

        if (!response.ok) {
            const errorText = await response.text();
            throw new Error(`Erreur HTTP: ${response.status} - ${response.statusText}. D√©tails: ${errorText}`);
        }

        if (!response.body) {
            throw new Error('Pas de body dans la r√©ponse');
        }

        return this.parseStreamResponse(response.body);
    }

    private async enrichMessageWithWorkspaceContext(message: string): Promise<string> {
        // Ajouter du contexte de workspace au message
        let context = "\n\n=== CONTEXTE WORKSPACE ===\n";
        
        try {
            // Informations de base sur le workspace
            const workspaceInfo = await this.getBasicWorkspaceInfo();
            context += `Workspace: ${workspaceInfo.name}\n`;
            context += `Type de projet: ${workspaceInfo.type}\n`;
            context += `Fichiers principaux: ${workspaceInfo.mainFiles.join(', ')}\n`;
            
        } catch (error) {
            context += "Erreur lors de la r√©cup√©ration du contexte workspace\n";
        }
        
        context += "=== FIN CONTEXTE ===\n\n";
        
        return context + message;
    }

    private async getBasicWorkspaceInfo(): Promise<any> {
        const vscode = require('vscode');
        const fs = require('fs');
        const path = require('path');
        
        const workspacePath = vscode.workspace.workspaceFolders?.[0]?.uri.fsPath;
        if (!workspacePath) {
            return { name: 'Inconnu', type: 'Inconnu', mainFiles: [] };
        }

        const name = path.basename(workspacePath);
        let type = 'Inconnu';
        const mainFiles: string[] = [];

        // D√©tecter le type de projet
        if (fs.existsSync(path.join(workspacePath, 'package.json'))) {
            type = 'Node.js/JavaScript';
            mainFiles.push('package.json');
        }
        if (fs.existsSync(path.join(workspacePath, 'tsconfig.json'))) {
            type = 'TypeScript';
            mainFiles.push('tsconfig.json');
        }
        if (fs.existsSync(path.join(workspacePath, 'src'))) {
            mainFiles.push('src/');
        }

        return { name, type, mainFiles };
    }

    private async* parseStreamResponse(body: ReadableStream<Uint8Array>): AsyncIterable<any> {
        const reader = body.getReader();
        const decoder = new TextDecoder();

        try {
            while (true) {
                const { done, value } = await reader.read();
                if (done) break;

                const chunk = decoder.decode(value, { stream: true });
                const lines = chunk.split('\n').filter(line => line.trim());

                for (const line of lines) {
                    try {
                        const parsed = JSON.parse(line);
                        yield parsed;
                    } catch (e) {
                        // Ignorer les lignes qui ne sont pas du JSON valide
                        console.warn('Ligne non-JSON ignor√©e:', line);
                    }
                }
            }
        } finally {
            reader.releaseLock();
        }
    }

    // Chat avec support des tools - version compl√®te avec callbacks
    async chatWithTools(
        message: string, 
        history: any[] = [],
        onToken: (token: string) => void, 
        onComplete: (fullResponse: string) => void,
        onToolCall: (toolCall: any) => Promise<any>
    ): Promise<void> {
        const config = this.getConfig();
        
        let modelToUse = config.model;
        if (!modelToUse || modelToUse === '') {
            modelToUse = await this.getAutoSelectedModel();
        }

        console.log(`üõ†Ô∏è Chat avec tools utilisant le mod√®le: ${modelToUse}`);

        try {
            const requestOptions = this.buildRequestOptions({});
            const tools = this.getAvailableTools();
            
            // Construire les messages avec l'historique
            const messages = [
                {
                    role: "system",
                    content: `Tu es un assistant de d√©veloppement avec acc√®s √† des outils pour analyser et manipuler le code. 
                    
Outils disponibles :
‚Ä¢ check_workspace() - Analyse l'espace de travail
‚Ä¢ check_directory(path) - Explore un r√©pertoire  
‚Ä¢ read_file(filepath) - Lit un fichier
‚Ä¢ find_functions(target, pattern) - Trouve les fonctions
‚Ä¢ check_dependencies() - Analyse les d√©pendances
‚Ä¢ generate_code(type, name, specifications) - G√©n√®re du code

Utilise ces outils pour r√©pondre aux questions sur le code et aider au d√©veloppement.`
                },
                ...history,
                {
                    role: "user", 
                    content: message
                }
            ];
            
            const response = await axios.post(`${config.serverUrl}/api/chat`, {
                model: modelToUse,
                messages: messages,
                tools: tools,
                stream: true,
                options: requestOptions
            }, {
                responseType: 'stream',
                timeout: 90000
            });

            let fullResponse = '';
            let buffer = '';

            response.data.on('data', async (chunk: Buffer) => {
                try {
                    buffer += chunk.toString();
                    const lines = buffer.split('\n');
                    buffer = lines.pop() || '';
                    
                    for (const line of lines) {
                        const trimmedLine = line.trim();
                        if (trimmedLine) {
                            try {
                                const data = JSON.parse(trimmedLine);
                                
                                // Gestion des tool calls
                                if (data.message?.tool_calls) {
                                    for (const toolCall of data.message.tool_calls) {
                                        console.log(`üîß Ex√©cution de l'outil: ${toolCall.function.name}`);
                                        const toolResult = await onToolCall(toolCall);
                                        console.log(`‚úÖ R√©sultat outil: ${JSON.stringify(toolResult).substring(0, 100)}...`);
                                    }
                                }
                                
                                // Gestion du contenu normal
                                if (data.message?.content) {
                                    fullResponse += data.message.content;
                                    onToken(data.message.content);
                                }
                                
                                if (data.done) {
                                    onComplete(fullResponse);
                                }
                            } catch (parseError) {
                                console.debug(`Ligne ignor√©e: ${trimmedLine.substring(0, 50)}...`);
                            }
                        }
                    }
                } catch (error) {
                    console.warn('Erreur traitement chunk:', error);
                }
            });

            response.data.on('end', () => {
                if (buffer.trim()) {
                    try {
                        const data = JSON.parse(buffer.trim());
                        if (data.message?.content) {
                            fullResponse += data.message.content;
                            onToken(data.message.content);
                        }
                        if (data.done) {
                            onComplete(fullResponse);
                        }
                    } catch (error) {
                        console.debug('Buffer final ignor√©');
                    }
                }
                if (fullResponse) {
                    onComplete(fullResponse);
                }
            });

        } catch (error) {
            if (axios.isAxiosError(error)) {
                throw new Error(`Erreur API Ollama tools: ${error.message}`);
            }
            throw error;
        }
    }
}
