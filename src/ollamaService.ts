import axios from 'axios';
import * as vscode from 'vscode';

interface ChatMessage {
    role: 'user' | 'assistant';
    content: string;
}
interface OllamaModel {
    name: string;
    modified_at: string;
    size: number;
}


export class OllamaService {
    private getConfig() {
        const config = vscode.workspace.getConfiguration('ollama');
        return {
            serverUrl: config.get('serverUrl', 'http://localhost:11434'),
            model: config.get('model', ''), // Pas de mod√®le par d√©faut, sera auto-d√©tect√©
            disableThinking: config.get('disableThinking', true) // Par d√©faut, d√©sactiver le "thinking"
        };
    }

    // Helper pour construire les options avec le contr√¥le du "thinking"
    private buildRequestOptions(baseOptions: any): any {
        const config = this.getConfig();
        const options = { ...baseOptions };
        
        if (config.disableThinking) {
            options.no_thinking = true;
        }
        
        return options;
    }
    
    // Helper pour savoir si le no_thinking est activ√©
    private isThinkingDisabled(): boolean {
        return this.getConfig().disableThinking;
    }

    // Helper pour debug optionnel du streaming
    private debugChunk(chunk: string, context: string): void {
        const config = vscode.workspace.getConfiguration('ollama');
        const debugMode = config.get('debugStreaming', false);
        
        if (debugMode) {
            console.log(`[${context}] Chunk re√ßu (${chunk.length} chars):`, chunk.substring(0, 100));
            
            // V√©rifier si c'est du JSON valide
            try {
                JSON.parse(chunk);
                console.log(`[${context}] ‚úÖ JSON valide`);
            } catch {
                console.log(`[${context}] ‚ùå JSON invalide`);
            }
        }
    }

    // NOUVELLE M√âTHODE: Obtenir automatiquement un mod√®le disponible
    async getAutoSelectedModel(): Promise<string> {
        try {
            const models = await this.getModels();
            
            if (models.length === 0) {
                throw new Error('Aucun mod√®le Ollama install√©. Suggestions:\n' +
                    '- ollama pull qwen2.5-coder:7b (sp√©cialis√© code)\n' +
                    '- ollama pull llama3.2:3b (l√©ger et rapide)\n' +
                    '- ollama pull phi3:mini (tr√®s l√©ger)');
            }
            
            // Ordre de pr√©f√©rence pour les mod√®les (m√™me logique que l'orchestrateur)
            const preferenceOrder = [
                'qwen2.5-coder:32b', 'qwen2.5-coder:14b', 'qwen2.5-coder:7b', 'qwen2.5-coder:3b',
                'codellama:34b', 'codellama:13b', 'codellama:7b', 'codellama:7b-code',
                'deepseek-coder:33b', 'deepseek-coder:6.7b', 'deepseek-coder:1.3b',
                'llama3.1:70b', 'llama3.1:8b', 'llama3.2:3b', 'llama3.2:1b',
                'gemma2:27b', 'gemma2:9b', 'gemma2:2b', 'mistral:7b', 'phi3:14b', 'phi3:mini'
            ];
            
            // Chercher le premier mod√®le disponible selon l'ordre de pr√©f√©rence
            for (const preferredModel of preferenceOrder) {
                const found = models.find(m => 
                    m.name === preferredModel || 
                    m.name.startsWith(preferredModel.split(':')[0])
                );
                
                if (found) {
                    console.log(`Mod√®le auto-s√©lectionn√©: ${found.name}`);
                    return found.name;
                }
            }
            
            // Si aucun mod√®le pr√©f√©r√© n'est trouv√©, prendre le premier disponible
            const fallbackModel = models[0].name;
            console.log(`Aucun mod√®le pr√©f√©r√© trouv√©, utilisation de: ${fallbackModel}`);
            return fallbackModel;
        } catch (error) {
            throw new Error(`D√©tection mod√®le √©chou√©e: ${error}`);
        }
    }

    async chat(message: string, history: ChatMessage[]): Promise<string> {
        const config = this.getConfig();
        
        // Si pas de mod√®le configur√©, d√©tecter automatiquement
        let modelToUse = config.model;
        if (!modelToUse) {
            console.log('Aucun mod√®le sp√©cifi√©, auto-s√©lection...');
            try {
                modelToUse = await this.getAutoSelectedModel();
            } catch (error) {
                throw error;
            }
        } else {
            // V√©rifier que le mod√®le configur√© existe
            try {
                const models = await this.getModels();
                const modelExists = models.some(m => m.name === modelToUse);
                
                if (!modelExists) {
                    console.warn(`Mod√®le ${modelToUse} introuvable, recherche d'une alternative...`);
                    modelToUse = await this.getAutoSelectedModel();
                }
            } catch (error) {
                console.warn('Impossible de v√©rifier les mod√®les, tentative avec le mod√®le configur√©...');
            }
        }

        console.log(`Utilisation du mod√®le: ${modelToUse}`);
        
        try {
            // Construire le contexte de conversation
            let conversation = '';
            if (history.length > 1) {
                // Prendre les derniers messages (limiter pour √©viter les tokens excessifs)
                const recentHistory = history.slice(-10);
                conversation = recentHistory
                    .slice(0, -1) // Exclure le dernier message (current)
                    .map(msg => `${msg.role === 'user' ? 'Utilisateur' : 'Assistant'}: ${msg.content}`)
                    .join('\n\n');
                conversation += '\n\n';
            }

            const fullPrompt = conversation + `Utilisateur: ${message}\nAssistant:`;

            const response = await axios.post(`${config.serverUrl}/api/generate`, {
                model: modelToUse,
                prompt: fullPrompt,
                stream: false,
                options: this.buildRequestOptions({
                    temperature: 0.7,
                    num_predict: 1000
                }),
                no_thinking: this.isThinkingDisabled() // Ajout direct du param√®tre no_thinking
            }, {
                timeout: 180000 // Augment√© pour les mod√®les lourds
            });

            if (response.data?.response) {
                return response.data.response.trim();
            } else {
                throw new Error('R√©ponse invalide d\'Ollama');
            }

        } catch (error) {
            if (axios.isAxiosError(error)) {
                if (error.code === 'ECONNREFUSED') {
                    throw new Error(`Impossible de se connecter √† Ollama sur ${config.serverUrl}`);
                } else if (error.response?.status === 404) {
                    throw new Error(`Mod√®le '${modelToUse}' introuvable`);
                } else {
                    throw new Error(`Erreur API Ollama: ${error.message}`);
                }
            } else {
                throw new Error(`Erreur inattendue: ${error}`);
            }
        }
    }

    // üöÄ NOUVELLE M√âTHODE: Chat avec streaming token par token
    async chatStream(
        message: string, 
        history: ChatMessage[],
        onToken: (token: string) => void,
        onComplete: (fullResponse: string) => void,
        onError: (error: string) => void
    ): Promise<void> {
        const config = this.getConfig();
        
        // Auto-s√©lection du mod√®le (m√™me logique que chat())
        let modelToUse = config.model;
        if (!modelToUse) {
            console.log('Aucun mod√®le sp√©cifi√© pour streaming, auto-s√©lection...');
            try {
                modelToUse = await this.getAutoSelectedModel();
            } catch (error) {
                onError(`D√©tection mod√®le √©chou√©e: ${error}`);
                return;
            }
        } else {
            try {
                const models = await this.getModels();
                const modelExists = models.some(m => m.name === modelToUse);
                
                if (!modelExists) {
                    console.warn(`Mod√®le ${modelToUse} introuvable pour streaming, recherche d'une alternative...`);
                    modelToUse = await this.getAutoSelectedModel();
                }
            } catch (error) {
                console.warn('Impossible de v√©rifier les mod√®les pour streaming, tentative avec le mod√®le configur√©...');
            }
        }

        console.log(`Utilisation du mod√®le pour streaming: ${modelToUse}`);
        
        try {
            // Construire le contexte de conversation
            let conversation = '';
            if (history.length > 1) {
                const recentHistory = history.slice(-10);
                conversation = recentHistory
                    .slice(0, -1)
                    .map(msg => `${msg.role === 'user' ? 'Utilisateur' : 'Assistant'}: ${msg.content}`)
                    .join('\n\n');
                conversation += '\n\n';
            }

            const fullPrompt = conversation + `Utilisateur: ${message}\nAssistant:`;

            const response = await axios.post(`${config.serverUrl}/api/generate`, {
                model: modelToUse,
                prompt: fullPrompt,
                stream: true, // Activer le streaming
                options: this.buildRequestOptions({
                    temperature: 0.7,
                    num_predict: 1000
                }),
                no_thinking: this.isThinkingDisabled() // Ajout direct du param√®tre no_thinking
            }, {
                timeout: 180000,
                responseType: 'stream'
            });

            let fullResponse = '';
            let buffer = ''; // Buffer pour accumuler les chunks partiels
            
            response.data.on('data', (chunk: Buffer) => {
                try {
                    // Convertir le chunk en string et l'ajouter au buffer
                    buffer += chunk.toString();
                    
                    // S√©parer les lignes compl√®tes
                    const lines = buffer.split('\n');
                    
                    // Garder la derni√®re ligne (potentiellement incompl√®te) dans le buffer
                    buffer = lines.pop() || '';
                    
                    // Traiter chaque ligne compl√®te
                    lines.forEach(line => {
                        const trimmedLine = line.trim();
                        if (trimmedLine) {
                            try {
                                const data = JSON.parse(trimmedLine);
                                
                                if (data.response) {
                                    fullResponse += data.response;
                                    onToken(data.response); // üöÄ Callback token par token
                                }
                                
                                if (data.done) {
                                    onComplete(fullResponse.trim()); // üéØ Callback compl√©tion
                                }
                            } catch (parseError) {
                                // Ignorer silencieusement les lignes non-JSON ou mal form√©es
                                // console.debug(`Ligne ignor√©e (non-JSON): ${trimmedLine.substring(0, 50)}...`);
                            }
                        }
                    });
                } catch (error) {
                    console.warn('Erreur traitement chunk streaming:', error);
                    // Ne pas interrompre le streaming pour une erreur de chunk
                }
            });

            response.data.on('end', () => {
                // Traiter le buffer restant s'il y en a un
                if (buffer.trim()) {
                    try {
                        const data = JSON.parse(buffer.trim());
                        if (data.response) {
                            fullResponse += data.response;
                            onToken(data.response);
                        }
                        if (data.done) {
                            onComplete(fullResponse.trim());
                        }
                    } catch (error) {
                        // Buffer final ignor√© s'il n'est pas du JSON valide
                    }
                }
                
                // S'assurer que onComplete est appel√© m√™me si pas de 'done' re√ßu
                if (fullResponse && !fullResponse.includes('undefined')) {
                    onComplete(fullResponse.trim());
                }
            });

            response.data.on('error', (error: any) => {
                onError(`Erreur stream: ${error.message}`);
            });

        } catch (error) {
            if (axios.isAxiosError(error)) {
                if (error.code === 'ECONNREFUSED') {
                    onError(`Impossible de se connecter √† Ollama sur ${config.serverUrl}`);
                } else if (error.response?.status === 404) {
                    onError(`Mod√®le '${modelToUse}' introuvable`);
                } else {
                    onError(`Erreur API Ollama: ${error.message}`);
                }
            } else {
                onError(`Erreur inattendue: ${error}`);
            }
        }
    }

    // NOUVELLE M√âTHODE: Chat streaming pour l'orchestrateur
    async chatStreamForWorker(
        prompt: string,
        onToken: (token: string) => void,
        onComplete: (fullResponse: string) => void
    ): Promise<void> {
        const config = this.getConfig();
        
        // Utiliser exactement la m√™me logique que chat() pour √©viter les erreurs 404
        let modelToUse = config.model;
        if (!modelToUse) {
            console.log('Aucun mod√®le sp√©cifi√© pour worker, auto-s√©lection...');
            try {
                modelToUse = await this.getAutoSelectedModel();
            } catch (error) {
                throw new Error(`D√©tection mod√®le √©chou√©e pour worker: ${error}`);
            }
        } else {
            // V√©rifier que le mod√®le configur√© existe (m√™me logique que chat())
            try {
                const models = await this.getModels();
                const modelExists = models.some(m => m.name === modelToUse);
                
                if (!modelExists) {
                    console.warn(`Mod√®le ${modelToUse} introuvable pour worker, recherche d'une alternative...`);
                    modelToUse = await this.getAutoSelectedModel();
                }
            } catch (error) {
                console.warn('Impossible de v√©rifier les mod√®les pour worker, tentative avec le mod√®le configur√©...');
            }
        }

        console.log(`Worker utilise le mod√®le: ${modelToUse}`);
        
        try {
            const response = await axios.post(`${config.serverUrl}/api/generate`, {
                model: modelToUse,
                prompt: prompt,
                stream: true,
                options: this.buildRequestOptions({
                    temperature: 0.2, // Plus bas pour consistance
                    num_predict: 500,
                    num_ctx: 2048
                }),
                no_thinking: this.isThinkingDisabled() // Ajout direct du param√®tre no_thinking
            }, {
                timeout: 90000,
                responseType: 'stream'
            });

            let fullResponse = '';
            let buffer = ''; // Buffer pour accumuler les chunks partiels
            
            response.data.on('data', (chunk: Buffer) => {
                try {
                    // Convertir le chunk en string et l'ajouter au buffer
                    buffer += chunk.toString();
                    
                    // S√©parer les lignes compl√®tes
                    const lines = buffer.split('\n');
                    
                    // Garder la derni√®re ligne (potentiellement incompl√®te) dans le buffer
                    buffer = lines.pop() || '';
                    
                    // Traiter chaque ligne compl√®te
                    lines.forEach(line => {
                        const trimmedLine = line.trim();
                        if (trimmedLine) {
                            try {
                                const data = JSON.parse(trimmedLine);
                                
                                if (data.response) {
                                    fullResponse += data.response;
                                    onToken(data.response);
                                }
                                
                                if (data.done) {
                                    onComplete(fullResponse.trim());
                                }
                            } catch (parseError) {
                                // Ignorer silencieusement les erreurs de parsing JSON
                                // console.debug(`Worker chunk ignor√©: ${trimmedLine.substring(0, 30)}...`);
                            }
                        }
                    });
                } catch (error) {
                    console.warn('Erreur worker streaming chunk:', error);
                    // Ne pas interrompre le streaming pour une erreur de chunk
                }
            });

            response.data.on('end', () => {
                // Traiter le buffer restant s'il y en a un
                if (buffer.trim()) {
                    try {
                        const data = JSON.parse(buffer.trim());
                        if (data.response) {
                            fullResponse += data.response;
                            onToken(data.response);
                        }
                        if (data.done) {
                            onComplete(fullResponse.trim());
                        }
                    } catch (error) {
                        // Buffer final ignor√© s'il n'est pas du JSON valide
                    }
                }
                
                // S'assurer que onComplete est appel√©
                if (fullResponse) {
                    onComplete(fullResponse.trim());
                }
            });

            response.data.on('error', (error: any) => {
                console.error('Erreur worker stream:', error);
                onComplete(fullResponse || 'Erreur streaming worker');
            });

        } catch (error) {
            throw new Error(`Erreur streaming: ${error}`);
        }
    }

    async testConnection(): Promise<boolean> {
        const config = this.getConfig();
        try {
            await axios.get(`${config.serverUrl}/api/tags`, { timeout: 5000 });
            return true;
        } catch {
            return false;
        }
    }
    async getModels(): Promise<OllamaModel[]> {
        const config = this.getConfig();
        try {
            const response = await axios.get(`${config.serverUrl}/api/tags`, {
                timeout: 10000
            });
            
            return response.data?.models || [];
        } catch (error) {
            if (axios.isAxiosError(error)) {
                if (error.code === 'ECONNREFUSED') {
                    throw new Error(`Impossible de se connecter √† Ollama sur ${config.serverUrl}`);
                } else {
                    throw new Error(`Erreur lors de la r√©cup√©ration des mod√®les: ${error.message}`);
                }
            } else {
                throw new Error(`Erreur inattendue: ${error}`);
            }
        }
    }
}
